
@article{BlackburnComputationalsemantics2003,
  title = {Computational Semantics},
  journal = {Theoria: An International Journal for Theory, History and Foundations of Science},
  author = {Blackburn, Patrick and Bos, Johan},
  year = {2003},
  pages = {27--45},
  file = {/home/arildm/Zotero/storage/CJ9APB7P/BlackburnBos2003Theoria.pdf}
}

@article{ChatzikyriakidisTypeTheoryNatural2017,
  title = {Type {{Theory}} for {{Natural Language Semantics}}},
  author = {Chatzikyriakidis, Stergios and Cooper, Robin},
  year = {2017},
  keywords = {TTR,type theory},
  file = {/home/arildm/Zotero/storage/EJ6HP6GP/Chatzikyriakidis Cooper 2017 type-theory-natural.pdf}
}

@article{CooperRecordsRecordTypes2005,
  title = {Records and {{Record Types}} in {{Semantic Theory}}},
  volume = {15},
  issn = {0955-792X, 1465-363X},
  doi = {10.1093/logcom/exi004},
  language = {en},
  number = {2},
  journal = {Journal of Logic and Computation},
  author = {Cooper, Robin},
  month = apr,
  year = {2005},
  keywords = {TTR},
  pages = {99-112},
  file = {/home/arildm/Zotero/storage/2FQHY3UG/Cooper 2005 Records_and_Record_Types_in_Semantic_Theory.pdf}
}

@article{CooperTypetheorysemantics2012,
  title = {Type Theory and Semantics in Flux},
  volume = {14},
  journal = {Handbook of the Philosophy of Science},
  author = {Cooper, Robin},
  year = {2012},
  keywords = {TTR,type theory},
  pages = {271--323},
  file = {/home/arildm/Zotero/storage/98QG5I2W/Cooper 2012 Type theory and semantics in flux.pdf}
}

@inproceedings{DobnikModellinglanguageaction2012,
  title = {Modelling Language, Action, and Perception in Type Theory with Records},
  booktitle = {International {{Workshop}} on {{Constraint Solving}} and {{Language Processing}}},
  publisher = {{Springer}},
  author = {Dobnik, Simon and Cooper, Robin and Larsson, Staffan},
  year = {2012},
  keywords = {TTR},
  pages = {70--91},
  file = {/home/arildm/Zotero/storage/FD4Z4XP3/Dobnik Cooper Larsson 2013 perceptual-ttr-post-proceedings-published-81140070.pdf}
}

@article{FernandoSituationsstrings2006,
  title = {Situations as Strings},
  volume = {165},
  journal = {Electronic Notes in Theoretical Computer Science},
  author = {Fernando, Tim},
  year = {2006},
  pages = {23--36},
  file = {/home/arildm/Zotero/storage/A3CLSDPV/Fernando 2006 Situations as strings.pdf}
}

@article{lspc,
  title = {Interfacing Language, Spatial Perception and Cognition in {{Type Theory}} with {{Records}}},
  volume = {5},
  number = {2},
  journal = {Journal of Language Modelling},
  author = {Dobnik, Simon and Cooper, Robin},
  year = {2017},
  keywords = {TTR,spatial relations},
  pages = {273--301},
  file = {/home/arildm/Zotero/storage/3SUGE6PL/lspc.pdf}
}

@book{martinlof84,
  title = {Intuitionistic Type Theory},
  volume = {9},
  publisher = {{Bibliopolis Napoli}},
  author = {Martin-L{\"o}f, Per and Sambin, Giovanni},
  year = {1984},
  keywords = {type theory},
  file = {/home/arildm/Zotero/storage/ZJRGURBB/Martin-LÃ¶f 1984 Intuitionistic type theory.pdf}
}

@article{CooperTypetheorylanguage2016,
  title = {Type Theory and Language: From Perception to Linguistic Communication},
  shorttitle = {Type Theory and Language},
  journal = {Draft of book chapters available from https://sites. google. com/site/typetheorywithrecords/drafts},
  author = {Cooper, Robin},
  year = {2016},
  keywords = {TTR},
  file = {/home/arildm/Zotero/storage/KWNW8W58/ttl161130.pdf}
}

@inproceedings{Larssonformalviewcorrective2009,
  title = {Towards a Formal View of Corrective Feedback},
  booktitle = {Proceedings of the {{EACL}} 2009 {{Workshop}} on {{Cognitive Aspects}} of {{Computational Language Acquisition}}},
  publisher = {{Association for Computational Linguistics}},
  author = {Larsson, Staffan and Cooper, Robin},
  year = {2009},
  keywords = {TTR,dialogue},
  pages = {1--9},
  file = {/home/arildm/Zotero/storage/QUQ5MXF6/Larsson and Cooper - 2009 - Towards a Formal View of Corrective Feedback.pdf}
}

@misc{CooperSemanticsactionperception2017,
  title = {Semantics, Action and Perception - an Overview of {{TTR}}},
  author = {Cooper, Robin},
  year = {2017},
  keywords = {TTR},
  file = {/home/arildm/Zotero/storage/DYAYM47Y/eslp cooper ttr-lect-slides17.pdf}
}

@incollection{DybjerIntuitionisticTypeTheory2016,
  edition = {Winter 2016},
  title = {Intuitionistic {{Type Theory}}},
  abstract = {Intuitionistic type theory (also constructive type theory orMartin-L{\"o}f type theory) is a formal logical system and philosophicalfoundation for constructive mathematics. It is afull-scale system which aims to play a similar role for constructivemathematics as Zermelo-Fraenkel Set Theory does forclassical mathematics. It is based on the propositions-as-typesprinciple and clarifies the Brouwer-Heyting-Kolmogorov interpretationof intuitionistic logic. It extends this interpretation to the moregeneral setting of intuitionistic type theory and thus provides ageneral conception not only of what a constructive proof is, but alsoof what a constructive mathematical object is. The main idea is thatmathematical concepts such as elements, sets and functions areexplained in terms of concepts from programming such as datastructures, data types and programs. This article describes the formalsystem of intuitionistic type theory and its semantic foundations., In this entry, we first give an overview of the most importantaspects of intuitionistic type theory\textemdash{}a kind of ``extendedabstract''. It is meant for a reader who is already somewhatfamiliar with the theory. Section 2 on the other hand, is meant for areader who is new to intuitionistic type theory but familiar withtraditional logic, including propositional and predicate logic,arithmetic, and set theory. Here we informally introduce severalaspects which distinguishes intuitionistic type theory from thesetraditional theories. In Section 3 we present a basic version of thetheory, close to Martin-L{\"o}f's first published version from1972. The reader who was intrigued by the informality of Section 2will now see in detail how the theory is built up. Section 4 thenpresents a number of important extensions of the basic theory. Inparticular, it emphasizes the central role of inductive (andinductive-recursive) definitions. Section 5 introduces the underlyingphilosophical ideas including the theory of meaning developed byMartin-L{\"o}f. While Section 5 is about philosophy and foundations,Section 6 gives an overview of mathematical models of the theory. InSection 7 finally, we describe several important variations of thecore Martin-L{\"o}f ``intensional'' theory described inSection 3 and 4.},
  booktitle = {The {{Stanford Encyclopedia}} of {{Philosophy}}},
  publisher = {{Metaphysics Research Lab, Stanford University}},
  author = {Dybjer, Peter and Palmgren, Erik},
  editor = {Zalta, Edward N.},
  year = {2016},
  keywords = {type theory}
}

@article{DBLP:journals/corr/HeZRS15,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1512.03385},
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  volume = {abs/1512.03385},
  journal = {CoRR},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  biburl = {http://dblp.org/rec/bib/journals/corr/HeZRS15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{DBLP:journals/corr/Graves13,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1308.0850},
  title = {Generating {{Sequences With Recurrent Neural Networks}}},
  volume = {abs/1308.0850},
  journal = {CoRR},
  author = {Graves, Alex},
  year = {2013},
  biburl = {http://dblp.org/rec/bib/journals/corr/Graves13},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Hochreiter:1997:LSM:1246443.1246450,
  title = {Long {{Short}}-{{Term Memory}}},
  volume = {9},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  number = {8},
  journal = {Neural Comput.},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  month = nov,
  year = {1997},
  pages = {1735-1780},
  publisher = {{MIT Press}},
  location = {Cambridge, MA, USA},
  issue_date = {November 15, 1997},
  numpages = {46},
  acmid = {1246450}
}

@article{DBLP:journals/corr/MalinowskiRF15,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1505.01121},
  title = {Ask {{Your Neurons}}: {{A Neural}}-Based {{Approach}} to {{Answering Questions}} about {{Images}}},
  volume = {abs/1505.01121},
  journal = {CoRR},
  author = {Malinowski, Mateusz and Rohrbach, Marcus and Fritz, Mario},
  year = {2015},
  biburl = {http://dblp.org/rec/bib/journals/corr/MalinowskiRF15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{imagenet_cvpr09,
  title = {{{ImageNet}}: {{A Large}}-{{Scale Hierarchical Image Database}}},
  booktitle = {{{CVPR09}}},
  author = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
  year = {2009},
  bibsource = {http://www.image-net.org/papers/imagenet<sub>c</sub>vpr09.bib}
}

@article{Detectron2018,
  title = {Detectron},
  author = {Girshick, Ross and Radosavovic, Ilija and Gkioxari, Georgia and Doll{\'a}r, Piotr and He, Kaiming},
  year = {2018}
}

@misc{pyttr,
  title = {{{PyTTR}}},
  author = {Cooper, Robin},
  year = {2017},
  keywords = {TTR}
}

@article{RantaTypeTheoryUniversal2006,
  title = {Type {{Theory}} and {{Universal Grammar}}},
  copyright = {Tous droits r{\'e}serv{\'e}s},
  issn = {1281-2463},
  doi = {10.4000/philosophiascientiae.415},
  abstract = {The paper takes a look at the history of the idea of universal grammar and compares it with multilingual grammars, as formalized in the Grammatical Framework, GF. The constructivist idea of formalizing math$\-$ematics piece by piece, in a weak logical framework, rather than trying to reduce everything to one single strong theory, is the model that guides the development of grammars in GF.},
  language = {en},
  number = {CS 6},
  journal = {Philosophia Scienti{\ae}. Travaux d'histoire et de philosophie des sciences},
  author = {Ranta, Aarne},
  month = sep,
  year = {2006},
  pages = {115-131},
  file = {/home/arildm/Zotero/storage/AUFCFYVC/Ranta - 2006 - Type Theory and Universal Grammar.pdf;/home/arildm/Zotero/storage/MFG7YLYP/415.html}
}

@article{AndreasLearningComposeNeural2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1601.01705},
  primaryClass = {cs},
  title = {Learning to {{Compose Neural Networks}} for {{Question Answering}}},
  abstract = {We describe a question answering model that applies to both images and structured knowledge bases. The model uses natural language strings to automatically assemble neural networks from a collection of composable modules. Parameters for these modules are learned jointly with network-assembly parameters via reinforcement learning, with only (world, question, answer) triples as supervision. Our approach, which we term a dynamic neural model network, achieves state-of-the-art results on benchmark datasets in both visual and structured domains.},
  journal = {arXiv:1601.01705 [cs]},
  author = {Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Klein, Dan},
  month = jan,
  year = {2016},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing},
  file = {/home/arildm/Zotero/storage/W5Q4VZDY/Andreas et al. - 2016 - Learning to Compose Neural Networks for Question A.pdf;/home/arildm/Zotero/storage/A3UIGGIU/1601.html}
}

@article{RedmonYouOnlyLook2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.02640},
  primaryClass = {cs},
  title = {You {{Only Look Once}}: {{Unified}}, {{Real}}-{{Time Object Detection}}},
  shorttitle = {You {{Only Look Once}}},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  journal = {arXiv:1506.02640 [cs]},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  month = jun,
  year = {2015},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/arildm/Zotero/storage/2WVYT8AG/Redmon et al. - 2015 - You Only Look Once Unified, Real-Time Object Dete.pdf;/home/arildm/Zotero/storage/XUNRDBS7/1506.html}
}

@article{yolo,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1612.08242},
  primaryClass = {cs},
  title = {{{YOLO9000}}: {{Better}}, {{Faster}}, {{Stronger}}},
  shorttitle = {{{YOLO9000}}},
  abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
  journal = {arXiv:1612.08242 [cs]},
  author = {Redmon, Joseph and Farhadi, Ali},
  month = dec,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/arildm/Zotero/storage/75R4TWJL/Redmon and Farhadi - 2016 - YOLO9000 Better, Faster, Stronger.pdf;/home/arildm/Zotero/storage/BARLE6W4/1612.html}
}

@article{RegierGroundingspatiallanguage2001a,
  title = {Grounding Spatial Language in Perception: {{An}} Empirical and Computational Investigation},
  volume = {130},
  copyright = {\textcopyright{} 2001, American Psychological Association},
  issn = {0096-3445},
  shorttitle = {Grounding Spatial Language in Perception},
  doi = {http://dx.doi.org.ezproxy.ub.gu.se/10.1037/0096-3445.130.2.273},
  abstract = {The present paper grounds the linguistic categorization of space in aspects of visual perception; specifically, the structure of projective spatial terms such as above are grounded in the process of attention and in vector-sum coding of overall direction. This is formalized in the attentional vector-sum (AVS) model. This computational model accurately predicts linguistic acceptability judgments for spatial terms, under a variety of spatial configurations. In 7 experiments, the predictions of the AVS model are tested against those of 3 competing models. The results support the AVS model and disconfirm its competitors. The authors conclude that the structure of linguistic spatial categories can be partially explained in terms of independently motivated perceptual processes.},
  language = {English},
  number = {2},
  journal = {Journal of Experimental Psychology: General},
  author = {Regier, Terry and Carlson, Laura A.},
  month = jun,
  year = {2001},
  keywords = {Orientation; Pattern Recognition; Visual; Semantics; Space Perception; Verbal Learning;,Attention,Classification (Cognitive Process) (major),Judgment,Linguistics (major),Mathematical Modeling (major),Spatial Perception (major),Visual Perception (major),Words (Phonetic Units)},
  pages = {273-298},
  file = {/home/arildm/Zotero/storage/WISAXKBR/Regier and Carlson - 2001 - Grounding spatial language in perception An empir.pdf}
}

@misc{LoganComputationalAnalysisApprehension1996,
  title = {A {{Computational Analysis}} of the {{Apprehension}} of {{Spatial Relations}}},
  abstract = {Basic, deictic, \& intrinsic spatial relations distinguished in the literature motivate the proposal that apprehension of spatial relations requires (1) spatial indexing to instantiate basic relations, (2) reference frame computation to adjust parameters of the representation, (3) spatial template alignment to specify scope of acceptability of a given relation, \& (4) assessment of goodness of fit of spatial templates. These representations \& processes may be combined in various orders to perform relation judgments, cuing tasks, \& verification tasks. Evidence from previous psychological studies corroborates spatial indexing \& reference frame computation. Four experiments provide new evidence of the computation of goodness of fit between position of the located object \& a spatial template representing the relation that is centered on \& aligned with the reference object. In a production task, Ss (N = 68 undergraduates) indicated regions of space corresponding to areas of greatest acceptability of 12 spatial relations. In a goodness rating task, Ss (N = 32) ranked areas of space as good, acceptable, \& bad examples of 10 spatial relations in sentences corresponding to pictures. Results showed consistent representations of each notion. In a similarity rating task, Ss (N = 101) evaluated equivalence of pairs from a set of 12 lexicalized spatial terms. Similarities posited for underlying conceptual templates account for resemblances in results of goodness \& similarity rating tasks. Finally, a spatial relation judgment task measured reaction times of Ss (N = 48) to stimuli that systematically varied the distance between reference \& located objects. Results showed that distance minimally affected times, thus supporting the idea that spatial templates applied in parallel, rather than serial visual routines, process spatial relationships. It is concluded that spatial templates are useful for description of many spatial relation meanings. 1 Table, 12 Figures, 39 References. L. Lucht},
  language = {eng},
  author = {Logan, Gordon and Sadler, Daniel},
  collaborator = {Logan, Gordon},
  month = jan,
  year = {1996},
  keywords = {4011,bookitem,Bookitem,Computational Linguistics (14100),Deixis (17750),Language Processing (43550),production /rating tasks,Production /Rating Tasks,Psycholinguistics; Theories and Models,Space (81600),spatial relations processing; computational analysis,Spatial Relations Processing; Computational Analysis},
  file = {/home/arildm/Zotero/storage/V3UFFU6V/Logan and Sadler - 1996 - A Computational Analysis of the Apprehension of Sp.pdf}
}

@unpublished{CooperAustinianTruthAttitudes,
  title = {Austinian {{Truth}}, {{Attitudes}} and {{Type Theory}} \$$\backslash$ast\$},
  author = {Cooper, Robin}
}

@article{CooperAustiniantruthattitudes2005,
  title = {Austinian Truth, Attitudes and Type Theory},
  language = {eng},
  author = {Cooper, Robin},
  year = {2005},
  keywords = {TTR,HUMANIORA och RELIGIONSVETENSKAP|SprÃ¥kvetenskap|LingvistikÃ¤mnen|Datorlingvistik,HUMANITIES and RELIGION|Languages and linguistics|Linguistic subjects|Computational linguistics,NATURAL SCIENCES|Computer and Information Science|Language Technology (Computational Linguistics)|Computational linguistics,NATURVETENSKAP|Data- och informationsvetenskap|SprÃ¥kteknologi (sprÃ¥kvetenskaplig databehandling)|Datorlingvistik},
  file = {/home/arildm/Downloads/Cooper 2005 Austinian.pdf}
}

@inproceedings{ttrspat,
  address = {Potsdam, Germany},
  title = {Spatial {{Descriptions}} in {{Type Theory}} with {{Records}}},
  booktitle = {Proceedings of {{IWCS}} 2013 {{Workshop}} on {{Computational Models}} of {{Spatial Language Interpretation}} and {{Generation}} ({{CoSLI}}-3)},
  publisher = {{Association for Computational Linguistics}},
  author = {Dobnik, Simon and Cooper, Robin},
  month = mar,
  year = {2013},
  keywords = {TTR},
  pages = {1--6},
  file = {/home/arildm/Zotero/storage/SXULHG3L/Dobnik and Cooper - 2013 - Spatial Descriptions in Type Theory with Records.pdf}
}

@inproceedings{Dobnik:2017ag,
  series = {CLASP Papers in Computational Linguistics},
  title = {Modular Mechanistic Networks: {{On}} Bridging Mechanistic and Phenomenological Models with Deep Neural Networks in Natural Language Processing},
  booktitle = {Proceedings of the {{Conference}} on {{Logic}} and {{Machine Learning}} in {{Natural Language}} ({{LaML}} 2017), {{Gothenburg}}, 12--13 {{June}} 2017},
  author = {Dobnik, Simon and Kelleher, John D.},
  pages = {1-11},
  file = {/home/arildm/Zotero/storage/52YJE6NF/mechanistic-phenomenological-models-in-nlp.pdf},
  crossref = {LaML-Proceeedings:2017}
}

@book{LaML-Proceeedings:2017,
  address = {Gothenburg, Sweden},
  title = {{{CLASP Papers}} in {{Computational Linguistics}}: {{Proceedings}} of the {{Conference}} on {{Logic}} and {{Machine Learning}} in {{Natural Language}} ({{LaML}} 2017), {{Gothenburg}}, 12 --13 {{June}}},
  volume = {1},
  publisher = {{CLASP, Centre for Language and Studies in Probability}},
  editor = {Dobnik, Simon and Lappin, Shalom},
  month = nov,
  year = {2017},
  file = {/home/arildm/Zotero/storage/63AK2K2T/Simon et al. - 2017 - CLASP Papers in Computational Linguistics.pdf;/home/arildm/Zotero/storage/CNCN6FDG/54911.html},
  organization = {Department of Philosophy, Linguistics and Theory of Science (FLOV), University of Gothenburg}
}

@article{RoySemioticschemasframework2005,
  title = {Semiotic Schemas: {{A}} Framework for Grounding Language in Action and Perception},
  volume = {167},
  issn = {0004-3702},
  shorttitle = {Semiotic Schemas},
  doi = {10.1016/j.artint.2005.04.007},
  abstract = {A theoretical framework for grounding language is introduced that provides a computational path from sensing and motor action to words and speech acts. The approach combines concepts from semiotics and schema theory to develop a holistic approach to linguistic meaning. Schemas serve as structured beliefs that are grounded in an agent's physical environment through a causal-predictive cycle of action and perception. Words and basic speech acts are interpreted in terms of grounded schemas. The framework reflects lessons learned from implementations of several language processing robots. It provides a basis for the analysis and design of situated, multimodal communication systems that straddle symbolic and non-symbolic realms.},
  language = {eng},
  number = {1},
  journal = {Artificial Intelligence},
  author = {Roy, Deb},
  year = {2005},
  keywords = {Action,Cross-Modal,Embodied,Grounding,Language,Meaning,Multimodal,Perception,Representation,Schemas,Semiotic,Situated},
  pages = {170--205},
  file = {/home/arildm/Zotero/storage/DA2BGULL/1-s2.0-S0004370205001037-main.pdf}
}

@inproceedings{PustejovskyPerceptualsemanticsconstruction1990,
  title = {Perceptual Semantics: The Construction of Meaning in Artificial Devices},
  shorttitle = {Perceptual Semantics},
  doi = {10.1109/ISIC.1990.128445},
  abstract = {The design of an artificial device that can acquire its own perceptually grounded meanings for internally represented control structure is discussed. A perceptual semantics, in which perceptual routines are mapped onto innate conceptual operators, is described. Entities and relations in the environment take on `meaning' for the device in three forms: the connotation, which is the internal encoding of an object within a syntactic discrimination lattice; the annotation, which is a procedural encoding of how connotations are translated into action/perception; and the denotation, which is the action associated with a procedure within the action-percept feedback loop. The author argues against model-theoretic interpretations for semantics of devices interacting within their environment, and in favor of pragmatically determined models of meaning},
  booktitle = {Proceedings. 5th {{IEEE International Symposium}} on {{Intelligent Control}} 1990},
  author = {Pustejovsky, J.},
  month = sep,
  year = {1990},
  keywords = {Grounding,action-percept feedback loop,adaptive control,annotation,artificial devices,Artificial intelligence,cognitive systems,Computer science,computer vision,conceptual representation,denotation,Encoding,feedback,Feedback loop,innate conceptual operators,Intelligent robots,Intelligent sensors,internal encoding,internally represented control structure,Lattices,learning systems,perceptual routines,perceptual semantics,perceptually grounded meanings,procedural encoding,Robot sensing systems,robot vision,robots,syntactic discrimination lattice,Testing},
  pages = {86-91 vol.1},
  file = {/home/arildm/Zotero/storage/7M4BCIIT/Pustejovsky - 1990 - Perceptual semantics the construction of meaning .pdf;/home/arildm/Zotero/storage/P692Y9RQ/128445.html}
}

@article{GuptaSurveyVisualQuestion2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.03865},
  primaryClass = {cs},
  title = {Survey of {{Visual Question Answering}}: {{Datasets}} and {{Techniques}}},
  shorttitle = {Survey of {{Visual Question Answering}}},
  abstract = {Visual question answering (or VQA) is a new and exciting problem that combines natural language processing and computer vision techniques. We present a survey of the various datasets and models that have been used to tackle this task. The first part of the survey details the various datasets for VQA and compares them along some common factors. The second part of this survey details the different approaches for VQA, classified into four types: non-deep learning models, deep learning models without attention, deep learning models with attention, and other models which do not fit into the first three. Finally, we compare the performances of these approaches and provide some directions for future work.},
  journal = {arXiv:1705.03865 [cs]},
  author = {Gupta, Akshay Kumar},
  month = may,
  year = {2017},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Artificial Intelligence},
  file = {/home/arildm/Zotero/storage/48GX3FA7/Gupta - 2017 - Survey of Visual Question Answering Datasets and .pdf;/home/arildm/Zotero/storage/IEZCNAWM/1705.html}
}

@article{CooperNegationdialogue2018,
  title = {Negation in Dialogue},
  abstract = {We consider the nature of negation in di-alogue as revealed by semantic phenomena such as negative dialogue particles, psycholin-guistic experimentation, and dialogue corpora. We examine alternative accounts of negation that can be used in TTR (Type Theory with Records), and conclude that an alternatives-based account which relates to the psycholog-ical notion of negation in simulation seman-tics is most appropriate. We show how this account relates to questions under discussion, dialogical relevance, and metalinguistic nega-tion.},
  author = {Cooper, Robin and Ginzburg, Jonathan},
  month = apr,
  year = {2018},
  file = {/home/arildm/Zotero/storage/W9MAYJGK/Negation_in_dialogue.pdf}
}

@article{Kampcalculusfirstorder1996,
  title = {A Calculus for First Order {{Discourse Representation Structures}}},
  volume = {5},
  issn = {0925-8531, 1572-9583},
  doi = {10.1007/BF00159343},
  abstract = {This paper presents a sound and complete proof system for the first order fragment of Discourse Representation Theory. Since the inferences that human language users draw from the verbal input they receive for the most transcend the capacities of such a system, it can be no more than a basis on which more powerful systems, which are capable of producing those inferences, may then be built. Nevertheless, even within the general setting of first order logic the structure of the ``formulas'' of DRS-languages, i.e. of the Discourse Representation Structures suggest for the components of such a system inference rules that differ somewhat from those usually found in proof systems for the first order predicate calculus and which are, we believe, more in keeping with inference patterns that are actually employed in common sense reasoning.This is why we have decided to publish the present exercise, in spite of the fact that it is not one for which a great deal of originality could be claimed. In fact, it could be argued that the problem addressed in this paper was solved when G{\"o}del first established the completeness of the system of Principia Mathematica for first order logic. For the DRS-languages we consider here are straightforwardly intertranslatable with standard formulations of the predicate calculus; in fact the translations are so straightforward that any sound and complete proof system for first order logic can be used as a sound and complete proof system for DRSs: simply translate the DRSs into formulas of predicate logic and then proceed as usual. As a matter of fact, this is how one has chosen to proceed in some implementations of DRT, which involve inferencing as well as semantic representation; an example is the Lex system developed jointly by IBM and the University of T{\"u}bingen (see in particular (Guenthner et al. 1986)).In the light of the close and simple connections between DRT and standard predicate logic, publication of what will be presented in this paper can be justified only in terms of the special mash we have tried to achieve between the general form and the particular rules of our proof system on the one hand and on the other the distinctive architecture of DRS-like semantic representation. Some additional justification is necessary, however, as there exist a number of other proof systems for first order DRT, some of which have pursued more or less the same aims that have motivated the system presented here. We are explicitly aware of those developed by (Koons 1988), (Saurer 1990), (Sedogbo and Eytan 1987), (Reinhart 1989), (Gabbay and Reyle 1994); perhaps there are others. (Sedogbo and Eytan 1987) is a tableau system, and (Reinhart 1989) and (Gabbay and Reyle 1994) are resolution based, goal directed. These systems may promise particular advantages when it comes to implementing inference engines operating on DRS-like premises. But they do not aim to conform to certain canons of actual inferencing by human interpreters of natural language; and indeed the proof procedures they propose depart quite drastically from what one could plausibly assume to go in the head of such an interpreter. Only (Koons 1988) and (Saurer 1990) are, like our system, inspired by the methods of natural deduction. But there are some differences in the choice of basic rules. In particular both (Koons 1988) and (Saurer 1990) have among their primitive rules the Rule of Reiteration, which permits the copying of a DRS condition from a DRS to any of its sub-DRSs. In our system this is a derived rule (see Section 4 below).We will develop our system in several stages. The necessary intuitions and the formal background are provided in Sections 1 and 2. (The formal definitions can be found also in the first two chapters of (Kamp and Reyle 1993). The first system we present is for a sublanguage of the one defined in Section 2, which differs from the full language in that it lacks identity and disjunction. The core of the paper consists of Section 3, where the proof system for this sublanguage is presented, and Section 5, which extends the system for the full language, including disjunctions (Section 5.1) and identity (Section 5.2) and then establishes soundness and completeness for the full system. Section 4 deals with certain derived inference principles.},
  language = {en},
  number = {3-4},
  journal = {Journal of Logic, Language and Information},
  author = {Kamp, Hans and Reyle, Uwe},
  month = oct,
  year = {1996},
  pages = {297-348},
  file = {/home/arildm/Zotero/storage/5D7W4D2U/Kamp and Reyle - 1996 - A calculus for first order Discourse Representatio.pdf;/home/arildm/Zotero/storage/YT33NP6W/BF00159343.html}
}

@incollection{GeurtsDiscourseRepresentationTheory2016,
  edition = {Spring 2016},
  title = {Discourse {{Representation Theory}}},
  abstract = {In the early 1980s, Discourse Representation Theory (DRT) wasintroduced by Hans Kamp as a theoretical framework for dealing withissues in the semantics and pragmatics of anaphora and tense (Kamp1981); a very similar theory was developed independently by Irene Heim(1982). The distinctive features of DRT, to be discussed below, arethat it is a mentalist and representationalist theory ofinterpretation, and that it is a theory of the interpretation not onlyof individual sentences but of discourse, as well. In these respectsDRT made a clear break with classical formal semantics, which duringthe 1970s had emanated from Montague's pioneering work (Thomason1974), but in other respects it continued the tradition, e.g., in itsuse of model-theoretical tools. In the meantime, DRT has come to serveas a framework for explaining a wide range of phenomena, but we willconfine our attention to fewer than a handful: anaphora, tense,presupposition, and propositional attitudes. For references to work onother topics, see the ``Further reading'' section.},
  booktitle = {The {{Stanford Encyclopedia}} of {{Philosophy}}},
  publisher = {{Metaphysics Research Lab, Stanford University}},
  author = {Geurts, Bart and Beaver, David I. and Maier, Emar},
  editor = {Zalta, Edward N.},
  year = {2016},
  keywords = {anaphora,compositionality,descriptions,indexicals,presupposition,semantics: dynamic},
  file = {/home/arildm/Zotero/storage/L6AH9LQL/discourse-representation-theory.html}
}

@book{CarlsonFunctionalFeaturesLanguage2004,
  title = {Functional {{Features}} in {{Language}} and {{Space}}},
  isbn = {978-0-19-926433-9},
  abstract = {There is much empirical evidence showing that factors other than the relative positions of objects in Euclidean space are important in the comprehension of a wide range of spatial prepositions in English and other languages. Yet thus far attempts at classifying what we will call extra-geometric constraints have not been forthcoming. In this chapter we survey the range of experimental evidence for extra-geometric constraints, and we provide the first attempt at a classification of these influences. We argue that extra-geometric influences are basically of two types: what we term dynamic-kinematic aspects of scenes, and knowledge of the functions of objects and how they usually interact with each other in particular situations. We review the evidence for each of these parameters across a range of types of preposition, and report some new data showing the influence of extrageometric variables on the comprehension of between. We conclude with a discussion of the implications the empirical data and resultant classification have for models of spatial language comprehension.},
  language = {en},
  publisher = {{Oxford University Press}},
  author = {Carlson, Laura and {van der Zee}, Emile},
  month = dec,
  year = {2004},
  doi = {10.1093/acprof:oso/9780199264339.001.0001}
}

@article{CoventryClassificationExtrageometricInfluences2004,
  title = {Towards a {{Classification}} of {{Extra}}-Geometric {{Influences}} on the {{Comprehension}} of {{Spatial Prepositions}}},
  doi = {10.1093/acprof:oso/9780199264339.003.0010},
  abstract = {There is much empirical evidence showing that factors other than the relative positions of objects in Euclidean space are important in the comprehension of a wide range of spatial prepositions in English and other languages. However, attempts at classifying so-called extra-geometric constraints have not been forthcoming. This chapter surveys experimental evidence for extra-geometric constraints, and provides the first attempt at classifying these influences. It argues that extra-geometric influences are basically of two types: dynamic-kinematic aspects of scenes, and knowledge of the functions of objects and how they usually interact with each other in particular situations. It reviews evidence for each of these parameters across a range of types of preposition, and reports some new data showing the influence of extra-geometric variables on the comprehension of between. The chapter concludes with a discussion of the implications the empirical data and resultant classification have for models of spatial language comprehensions. \textcopyright{} the several contributors, Laura Carlson and Emile van der Zee, 2005. All rights reserved.},
  author = {Coventry, Kenny and Garrod, Simon},
  month = dec,
  year = {2004},
  file = {/home/arildm/Zotero/storage/B58JEM2N/Carlson and van der Zee - 2004 - Functional Features in Language and Space.pdf}
}

@incollection{CoventrySpatialPrepositionsVague2005,
  address = {Berlin, Heidelberg},
  title = {Spatial {{Prepositions}} and {{Vague Quantifiers}}: {{Implementing}} the {{Functional Geometric Framework}}},
  volume = {3343},
  isbn = {978-3-540-25048-7 978-3-540-32255-9},
  shorttitle = {Spatial {{Prepositions}} and {{Vague Quantifiers}}},
  abstract = {There is much empirical evidence showing that factors other than the relative positions of objects in Euclidean space are important in the comprehension of a wide range of spatial prepositions in English and other languages. We first the overview the functional geometric framework (Coventry \& Garrod, 2004) which puts ``what'' and ``where'' information together to underpin the situation specific meaning of spatial terms. We then outline an implementation of this framework. The computational model for the processing of visual scenes and the identification of the appropriate spatial preposition consists of three main modules: (1) Vision Processing, (2) Elman Network, (3) Dual-Route Network. Mirroring data from experiments with human participants, we show that the model is both able to predict what will happen to objects in a scene, and use these judgements to influence the appropriateness of over/under/above/below to describe where objects are located in the scene. Extensions of the model to other prepositions and quantifiers are discussed.},
  language = {en},
  booktitle = {Spatial {{Cognition IV}}. {{Reasoning}}, {{Action}}, {{Interaction}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Coventry, Kenny R. and Cangelosi, Angelo and Rajapakse, Rohanna and Bacon, Alison and Newstead, Stephen and Joyce, Dan and Richards, Lynn V.},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Freksa, Christian and Knauff, Markus and Krieg-Br{\"u}ckner, Bernd and Nebel, Bernhard and Barkowsky, Thomas},
  year = {2005},
  pages = {98-110},
  file = {/home/arildm/Zotero/storage/862VDN3D/Coventry et al. - 2005 - Spatial Prepositions and Vague Quantifiers Implem.pdf},
  doi = {10.1007/978-3-540-32255-9_6}
}

@phdthesis{DobnikTeachingmobilerobots2009,
  title = {Teaching Mobile Robots to Use Spatial Words},
  school = {The Queen's College, University of Oxford},
  author = {Dobnik, Simon},
  year = {2009},
  file = {/home/arildm/Zotero/storage/NFA7MGRT/thesis.pdf}
}

@article{DiamantCognitiveimageprocessing2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1411.0054},
  primaryClass = {cs, q-bio},
  title = {Cognitive Image Processing: The Time Is Right to Recognize That the World Does Not Rest More on Turtles and Elephants},
  shorttitle = {Cognitive Image Processing},
  abstract = {Traditional image processing is a field of science and technology developed to facilitate human-centered image management. But today, when huge volumes of visual data inundate our surroundings (due to the explosive growth of image-capturing devices, proliferation of Internet communication means and video sharing services over the World Wide Web), human-centered handling of Big-data flows is impossible anymore. Therefore, it has to be replaced with a machine (computer) supported counterpart. Of course, such an artificial counterpart must be equipped with some cognitive abilities, usually characteristic for a human being. Indeed, in the past decade, a new computer design trend - Cognitive Computer development - is become visible. Cognitive image processing definitely will be one of its main duties. It must be specially mentioned that this trend is a particular case of a much more general movement - the transition from a "computational data-processing paradigm" to a "cognitive information-processing paradigm", which affects today many fields of science, technology, and engineering. This transition is a blessed novelty, but its success is hampered by the lack of a clear delimitation between the notion of data and the notion of information. Elaborating the case of cognitive image processing, the paper intends to clarify these important research issues.},
  journal = {arXiv:1411.0054 [cs, q-bio]},
  author = {Diamant, Emanuel},
  month = oct,
  year = {2014},
  keywords = {Computer Science - Computers and Society,Quantitative Biology - Neurons and Cognition},
  file = {/home/arildm/Zotero/storage/LMCE8WN8/Diamant - 2014 - Cognitive image processing the time is right to r.pdf;/home/arildm/Zotero/storage/NT2KDZSI/1411.html}
}

@misc{Howthingstyp,
  title = {{How to do things with typ\ldots{}}},
  language = {sv},
  howpublished = {http://earlymoderntown.gu.se/forskning/publikation/?languageId=100000\&disableRedirect=true\&returnUrl=http\%3A\%2F\%2Fearlymoderntown.gu.se\%2Fenglish\%2Fresearch\%2Fpublication\%2F\%3FlanguageId\%3D100001\%26publicationId\%3D203076\&publicationId=203076},
  journal = {G{\"o}teborgs universitet}
}

@incollection{CoquandTypeTheory2015,
  edition = {Summer 2015},
  title = {Type {{Theory}}},
  abstract = {The topic of type theory is fundamental both in logic and computerscience. We limit ourselves here to sketch some aspects that areimportant in logic. For the importance of types in computer science, werefer the reader for instance to Reynolds 1983 and 1985.},
  booktitle = {The {{Stanford Encyclopedia}} of {{Philosophy}}},
  publisher = {{Metaphysics Research Lab, Stanford University}},
  author = {Coquand, Thierry},
  editor = {Zalta, Edward N.},
  year = {2015},
  keywords = {category theory,Frege; Gottlob,Frege; Gottlob: theorem and foundations for arithmetic,logic: paraconsistent,mathematics: inconsistent,Peano; Giuseppe,Principia Mathematica,Russell; Bertrand,type theory: Church's type theory,type theory: intuitionistic}
}

@article{church40,
  title = {A {{Formulation}} of the {{Simple Theory}} of {{Types}}},
  volume = {5},
  issn = {0022-4812},
  doi = {10.2307/2266170},
  abstract = {The purpose of the present paper is to give a formulation of the simple theory of types which incorporates certain features of the calculus of $\lambda$-conversion. A complete incorporation of the calculus of $\lambda$-conversion into the theory of types is impossible if we require that $\lambda$x and juxtaposition shall retain their respective meanings as an abstraction operator and as denoting the application of function to argument. But the present partial incorporation has certain advantages from the point of view of type theory and is offered as being of interest on this basis (whatever may be thought of the finally satisfactory character of the theory of types as a foundation for logic and mathematics).For features of the formulation which are not immediately connected with the incorporation of $\lambda$-conversion, we are heavily indebted to Whitehead and Russell, Hilbert and Ackermann, Hilbert and Bernays, and to forerunners of these, as the reader familiar with the works in question will recognize.The class of type symbols is described by the rules that {\i} and o are each type symbols and that if $\alpha$ and $\beta$ are type symbols then ($\alpha\beta$) is a type symbol: it is the least class of symbols which contains the symbols {\i} and o and is closed under the operation of forming the symbol ($\alpha\beta$) from the symbols $\alpha$ and $\beta$.},
  language = {eng},
  number = {2},
  journal = {The Journal of Symbolic Logic},
  author = {Church, Alonzo},
  month = jun,
  year = {1940},
  keywords = {Philosophy ; Mathematics;},
  pages = {56--68},
  file = {/home/arildm/Zotero/storage/D762AY5V/Church - 1940 - A Formulation of the Simple Theory of Types.pdf}
}

@article{BarwiseSituationsAttitudes1981,
  title = {Situations and {{Attitudes}}},
  volume = {78},
  number = {11},
  journal = {Journal of Philosophy},
  author = {Barwise, Jon and Perry, John},
  year = {1981},
  pages = {668--691},
  file = {/home/arildm/Zotero/storage/5PKPFD2Y/Barwise and Perry - 1981 - Situations and Attitudes.pdf}
}

@article{H.ClarkSpacetimesemantics1973,
  title = {Space, Time, Semantics, and the Child.},
  abstract = {Discusses man's capabilities and limitations as an element in a closed loop control system under normal environmental conditions. Factors considered include the nature of manual control, modes of tracking, mathematical models of human operators, and characteristics of controls and displays in tracking tasks. (21/2 p ref) (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  journal = {Cognitive development and the acquisition of language},
  author = {H. Clark, Herbert},
  month = jan,
  year = {1973},
  file = {/home/arildm/Zotero/storage/3GCZM5EK/H. Clark - 1973 - Space, time, semantics, and the child..pdf}
}

@article{HarnadSymbolGroundingProblem1990,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {cs/9906002},
  title = {The {{Symbol Grounding Problem}}},
  volume = {42},
  issn = {01672789},
  doi = {10.1016/0167-2789(90)90087-6},
  abstract = {How can the semantic interpretation of a formal symbol system be made intrinsic to the system, rather than just parasitic on the meanings in our heads? How can the meanings of the meaningless symbol tokens, manipulated solely on the basis of their (arbitrary) shapes, be grounded in anything but other meaningless symbols? The problem is analogous to trying to learn Chinese from a Chinese/Chinese dictionary alone. A candidate solution is sketched: Symbolic representations must be grounded bottom-up in nonsymbolic representations of two kinds: (1) "iconic representations," which are analogs of the proximal sensory projections of distal objects and events, and (2) "categorical representations," which are learned and innate feature-detectors that pick out the invariant features of object and event categories from their sensory projections. Elementary symbols are the names of these object and event categories, assigned on the basis of their (nonsymbolic) categorical representations. Higher-order (3) "symbolic representations," grounded in these elementary symbols, consist of symbol strings describing category membership relations (e.g., "An X is a Y that is Z").},
  number = {1-3},
  journal = {Physica D: Nonlinear Phenomena},
  author = {Harnad, Stevan},
  month = jun,
  year = {1990},
  keywords = {Computer Science - Artificial Intelligence,I.2.0},
  pages = {335-346},
  file = {/home/arildm/Zotero/storage/CKSAVE8F/Harnad - 1990 - The Symbol Grounding Problem.pdf;/home/arildm/Zotero/storage/I6ZWDRJS/9906002.html}
}

@article{LarssonFormalsemanticsperceptual2015,
  title = {Formal Semantics for Perceptual Classification},
  volume = {25},
  issn = {0955-792X},
  doi = {10.1093/logcom/ext059},
  abstract = {A formal semantics for low-level perceptual aspects of meaning is presented, tying these together with the logical-inferential aspects of meaning traditionally studied in formal semantics. The key idea is to model perceptual meanings as classifiers of perceptual input . Furthermore, we show how perceptual aspects of meaning can be updated as a result of observing language use in interaction, thereby enabling fine-grained semantic plasticity and semantic coordination. This requires a framework where intensions are (i) represented independently of extensions, and (ii) structured objects which can be modified as a result of learning. We use Type Theory with Records (TTR), a formal semantics framework that starts from the idea that information and meaning is founded on our ability to perceive and classify the world, i.e. to perceive objects and situations as being of types. As an example of our approach, we show how a simple classifier of spatial information based on the Perceptron can be cast in TTR.},
  number = {2},
  journal = {Journal of Logic and Computation},
  author = {Larsson, Staffan},
  year = {2015},
  keywords = {dialogue,compositionality,formal semantics,learning,perception,statistical classifiers,symbol grounding,Type theoretic semantics},
  pages = {335--369},
  file = {/home/arildm/Zotero/storage/JMZAEC47/Larsson - 2015 - Formal semantics for perceptual classification.pdf;/home/arildm/Zotero/storage/CIF49XCM/954129.html}
}

@inproceedings{LarssonDialoguesHaveContent2011,
  title = {Do {{Dialogues Have Content}}?},
  volume = {6736},
  isbn = {978-3-642-22221-4},
  abstract = {In this paper, the notion of ``the content of a dialogue'' is shown to be problematic in light of the phenomena of semantic coordination in dialogue, and the associated notion of semantic plasticity \textendash{} the ability of meanings to change as a result of language use. Specifically, it appears that any notion of content in dialogue based on classical modeltheoretical semantics will be insufficient for capturing semantic plasticity. An alternative formal semantics, type theory with records (TTR) is briefly introduced and is show to be better equipped to deal with semantic coordination and plasticity. However, it is also argued that any account of content in dialogue which takes semantic coordination seriously will also need to consider the problems it raises for some concepts central to traditional notions of meaning, namely inference and truth.},
  language = {eng},
  booktitle = {Sylvain {{Pogodalla And Jean}}-{{Philippe Prost}}, {{Eds}}: {{Proceedings Of Logical Aspects Of Computational Linguistics}} ({{Lacl}} 2011), {{Springer Lecture Notes In Computer Science}}.},
  author = {Larsson, Staffan},
  year = {2011},
  keywords = {Filosofi,General Language Studies And Linguistics,Human Computer Interaction,JÃ¤mfÃ¶rande SprÃ¥kvetenskap Och Lingvistik,Language Technology (Computational Linguistics),Languages And Literature,MÃ¤nniska-Datorinteraktion (Interaktionsdesign),Philosophy,SprÃ¥k Och Litteratur,SprÃ¥kteknologi (SprÃ¥kvetenskaplig Databehandling)},
  file = {/home/arildm/Zotero/storage/HZWDHQWF/Larsson - 2011 - Do Dialogues Have Content.pdf}
}

@article{Garnhamunifiedtheorymeaning1989,
  title = {A Unified Theory of the Meaning of Some Spatial Relational Terms},
  volume = {31},
  issn = {0010-0277},
  doi = {10.1016/0010-0277(89)90017-6},
  abstract = {This paper presents a unified account of the meaning of the spatial relational terms right, left, in front of, behind, above and below. It claims that each term has three types of meanings, basic, deictic and intrinsic, and that the definitions of each type of meaning are identical in form for all six terms. Restrictions on the use of the terms, which are different for above and below than for the rest, are explained by a general constraint on all uses of spatial relational terms, the framework vertical constraint. This constraint depends on the existence of a fourth type of meaning for above and below, one defined by the framework in which the related objects are located. It is argued that a theory centred on the framework vertical constraint is preferable to one centred on the principle of canonical orientation (Levelt, 1984).
R{\'e}sum{\'e}
Cet article offre une description unifi{\'e}e du sens des termes d{\'e}crivant des relations spatiales ({\`a} droite de, {\`a} gauche de, devant, derri{\`e}re, au dessus de et au dessous de). L'article propose que chaque terme poss{\`e}de trois sortes de sens - sens de base, sens d{\'e}ictique et sens intrins{\`e}que - dont les d{\'e}finitions sont formellement identiques pour les six termes. Les restrictions dans l'usage de ces termes, qui sont diff{\'e}rentes pour au dessus de et au dessous de et pour les autres termes, s'expliquent par une contrainte g{\'e}n{\'e}rale {\`a} toute utilisation de termes de relations spatiales, la contrainte du cadre vertical. Cette contrainte d{\'e}pend de l'existence d'un quatri{\'e}me type de sens pour au dessus de et au dessous de, sens d{\'e}fini par le cadre spatial dans lequel se situent les objets. Des arguments sugg{\`e}rent qu'une th{\'e}orie fond{\'e}e sur la contrainte du cadre vertical est pr{\'e}f{\'e}rable {\`a} une th{\'e}orie fond{\'e}e sur le principe d'orientation canonique (Levelt, 1984).},
  number = {1},
  journal = {Cognition},
  author = {Garnham, Alan},
  month = feb,
  year = {1989},
  pages = {45-60},
  file = {/home/arildm/Zotero/storage/BXBYB84R/Garnham - 1989 - A unified theory of the meaning of some spatial re.pdf;/home/arildm/Zotero/storage/3Q2WVCPQ/0010027789900176.html}
}

@book{SteelsSymbolGroundingProblem2007,
  title = {The {{Symbol Grounding Problem}} Has Been Solved. {{So}} What's next?},
  abstract = {In the nineteen eighties, a lot of ink was spent on the question of symbol grounding, largely triggered by Searle's Chinese Room story (Searle,1980). Searle's article had the advantage of stirring up discussion about when and how symbols could be about things in the world, whether intelligence involves representations or not, and what embodiment means and under what conditions cognition is embodied. But almost twentyfive years of philosophical discussion have shed little light on the issue, partly because the discussion has been mixed up with emotional arguments whether artificial intelligence is possible or not. However today I believe that sufficient progress has been made in cognitive science and AI so that we can say that the symbol grounding problem has been solved. The paper briefly discusses the issues of symbols, meanings and embodiment, the main themes of the workshop, why I claim the symbol grounding problem has been solved, and what we should do next. 1},
  author = {Steels, Luc},
  year = {2007},
  file = {/home/arildm/Zotero/storage/ECDA4C2M/Steels - 2007 - The Symbol Grounding Problem has been solved. So w.pdf;/home/arildm/Zotero/storage/NXHJQXN7/summary.html}
}

@article{AgrawalVQAVisualQuestion2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1505.00468},
  primaryClass = {cs},
  title = {{{VQA}}: {{Visual Question Answering}}},
  shorttitle = {{{VQA}}},
  abstract = {We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing \textasciitilde{}0.25M images, \textasciitilde{}0.76M questions, and \textasciitilde{}10M answers (www.visualqa.org), and discuss the information it provides. Numerous baselines and methods for VQA are provided and compared with human performance. Our VQA demo is available on CloudCV (http://cloudcv.org/vqa).},
  journal = {arXiv:1505.00468 [cs]},
  author = {Agrawal, Aishwarya and Lu, Jiasen and Antol, Stanislaw and Mitchell, Margaret and Zitnick, C. Lawrence and Batra, Dhruv and Parikh, Devi},
  month = may,
  year = {2015},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/arildm/Zotero/storage/4JUAWGGR/Agrawal et al. - 2015 - VQA Visual Question Answering.pdf;/home/arildm/Zotero/storage/Y5IJDGNZ/1505.html}
}

@inproceedings{LoweObjectrecognitionlocal1999,
  title = {Object Recognition from Local Scale-Invariant Features},
  volume = {2},
  doi = {10.1109/ICCV.1999.790410},
  abstract = {An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds},
  booktitle = {Proceedings of the {{Seventh IEEE International Conference}} on {{Computer Vision}}},
  author = {Lowe, D. G.},
  year = {1999},
  keywords = {3D projection,blurred image gradients,candidate object matches,cluttered partially occluded images,computation time,computational geometry,Computer science,Electrical capacitance tomography,feature extraction,Filters,image matching,Image recognition,inferior temporal cortex,Layout,least squares approximations,Lighting,local geometric deformations,local image features,local scale-invariant features,low residual least squares solution,multiple orientation planes,nearest neighbor indexing method,Neurons,object recognition,Object recognition,primate vision,Programmable logic arrays,Reactive power,robust object recognition,staged filtering approach,unknown model parameters},
  pages = {1150-1157 vol.2},
  file = {/home/arildm/Zotero/storage/FW22NGTN/Lowe - 1999 - Object recognition from local scale-invariant feat.pdf;/home/arildm/Zotero/storage/8XCCJUU9/790410.html}
}

@incollection{BlaschkoLearningLocalizeObjects2008,
  address = {Berlin, Heidelberg},
  title = {Learning to {{Localize Objects}} with {{Structured Output Regression}}},
  volume = {5302},
  isbn = {978-3-540-88681-5 978-3-540-88682-2},
  abstract = {Sliding window classifiers are among the most successful and widely applied techniques for object localization. However, training is typically done in a way that is not specific to the localization task. First a binary classifier is trained using a sample of positive and negative examples, and this classifier is subsequently applied to multiple regions within test images. We propose instead to treat object localization in a principled way by posing it as a problem of predicting structured data: we model the problem not as binary classification, but as the prediction of the bounding box of objects located in images. The use of a joint-kernel framework allows us to formulate the training procedure as a generalization of an SVM, which can be solved efficiently. We further improve computational efficiency by using a branch-and-bound strategy for localization during both training and testing. Experimental evaluation on the PASCAL VOC and TU Darmstadt datasets show that the structured training procedure improves performance over binary training as well as the best previously published scores.},
  language = {en},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2008},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Blaschko, Matthew B. and Lampert, Christoph H.},
  editor = {Forsyth, David and Torr, Philip and Zisserman, Andrew},
  year = {2008},
  pages = {2-15},
  file = {/home/arildm/Zotero/storage/U826VJ7R/Blaschko and Lampert - 2008 - Learning to Localize Objects with Structured Outpu.pdf},
  doi = {10.1007/978-3-540-88682-2_2}
}

@article{HeDeepResidualLearning2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1512.03385},
  primaryClass = {cs},
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  journal = {arXiv:1512.03385 [cs]},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  month = dec,
  year = {2015},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/arildm/Zotero/storage/ASFG6L9P/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf;/home/arildm/Zotero/storage/ZZGBKWRC/1512.html}
}

@article{HornPerformanceConvolutionalNeural2017,
  series = {Control Conference Africa CCA 2017},
  title = {Performance of {{Convolutional Neural Networks}} for {{Feature Extraction}} in {{Froth Flotation Sensing}}},
  volume = {50},
  issn = {2405-8963},
  doi = {10.1016/j.ifacol.2017.12.003},
  abstract = {Image-based soft sensors are of interest in process industries due to their cost-effective and non-intrusive properties. Unlike most multivariate inputs, images are highly dimensional, requiring the use of feature extractors to produce lower dimension representations. These extractors have a large impact on final sensor performance. Traditional texture feature extraction methods consider limited feature types, requiring expert knowledge to select and may be sensitive to changing imaging conditions. Deep learning methods are an alternative which does not suffer these drawbacks. A specific deep learning method, Convolutional Neural Networks (CNNs), mitigates the curse of dimensionality inherent in fully connected networks but must be trained, unlike other feature extractors. This allows both textural and spectral features to be discovered and utilised. A case study consisting of platinum flotation froth images at four distinct platinum-grades was used. Extracted feature sets were used to train linear and nonlinear soft sensor models. The quality of CNN features was compared to those from traditional texture feature extraction methods. Performance of CNNs as feature extractors was found to be competitive, showing similar performance to the other texture feature extractors. However, the dataset also exhibits strong spectral features, complicating comparison between texture feature extractors. The results gathered do not provide sufficient information to distinguish between the types of features detected by the CNN and further investigation is required.},
  number = {2},
  journal = {IFAC-PapersOnLine},
  author = {Horn, Z. C. and Auret, L. and McCoy, J. T. and Aldrich, C. and Herbst, B. M.},
  month = dec,
  year = {2017},
  keywords = {computer vision,data reduction,machine learning,neural networks,soft sensing},
  pages = {13-18},
  file = {/home/arildm/Zotero/storage/ECTSM88W/Horn et al. - 2017 - Performance of Convolutional Neural Networks for F.pdf;/home/arildm/Zotero/storage/KMN485HT/S2405896317335346.html}
}

@inproceedings{koller2014:eccv,
  address = {Zurich, Switzerland},
  title = {Read {{My Lips}}: {{Continuous Signer Independent Weakly Supervised Viseme Recognition}}},
  booktitle = {Proceedings of the 13th {{European Conference}} on {{Computer Vision}}},
  author = {Koller, Oscar and Ney, Hermann and Bowden, Richard},
  month = sep,
  year = {2014},
  pages = {281-296},
  booktitlelink = {http://eccv2014.org/}
}

@inproceedings{koller15:mouth,
  address = {Santiago, Chile},
  title = {Deep {{Learning}} of {{Mouth Shapes}} for {{Sign Language}}},
  booktitle = {Third {{Workshop}} on {{Assistive Computer Vision}} and {{Robotics}}, {{ICCV}}},
  author = {Koller, Oscar and Ney, Hermann and Bowden, Richard},
  month = dec,
  year = {2015},
  pages = {477-483},
  aux = {https://www-i6.informatik.rwth-aachen.de/publications/downloader.php?id=997&row=aux}
}

@inproceedings{forster12:RWTH-PHOENIX-Weather,
  address = {Istanbul, Turkey},
  title = {{{RWTH}}-{{PHOENIX}}-{{Weather}}: {{A Large Vocabulary Sign Language Recognition}} and {{Translation Corpus}}},
  booktitle = {Language {{Resources}} and {{Evaluation}}},
  author = {Forster, Jens and Schmidt, Christoph and Hoyoux, Thomas and Koller, Oscar and Zelle, Uwe and Piater, Justus and Ney, Hermann},
  month = may,
  year = {2012},
  pages = {3785-3789},
  booktitlelink = {http://www.lrec-conf.org/lrec2012/}
}

@article{DBLP:journals/corr/HeZRS15,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1512.03385},
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  volume = {abs/1512.03385},
  journal = {CoRR},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  biburl = {http://dblp.org/rec/bib/journals/corr/HeZRS15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{DBLP:journals/corr/Graves13,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1308.0850},
  title = {Generating {{Sequences With Recurrent Neural Networks}}},
  volume = {abs/1308.0850},
  journal = {CoRR},
  author = {Graves, Alex},
  year = {2013},
  biburl = {http://dblp.org/rec/bib/journals/corr/Graves13},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Hochreiter:1997:LSM:1246443.1246450,
  title = {Long {{Short}}-{{Term Memory}}},
  volume = {9},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  number = {8},
  journal = {Neural Comput.},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  month = nov,
  year = {1997},
  pages = {1735-1780},
  publisher = {{MIT Press}},
  location = {Cambridge, MA, USA},
  issue_date = {November 15, 1997},
  numpages = {46},
  acmid = {1246450}
}

@article{doi:10.1044/jshr.1104.796,
  title = {Confusions {{Among Visually Perceived Consonants}}},
  volume = {11},
  doi = {10.1044/jshr.1104.796},
  number = {4},
  journal = {Journal of Speech, Language, and Hearing Research},
  author = {Fisher, Cletus G.},
  year = {1968},
  pages = {796-804}
}

@inproceedings{antonakos,
  address = {Lubljana, Slovenia},
  title = {A Survey on Mouth Modeling and Analysis for Sign Language Recognition.},
  booktitle = {2015 11th {{IEEE International Conference}} and {{Workshops}} on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Antonakos, E. and Roussos, A. and Zafeiriou, S.},
  month = may,
  year = {2015},
  pages = {1-7}
}

@article{HeMaskRCNN2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.06870},
  primaryClass = {cs},
  title = {Mask {{R}}-{{CNN}}},
  abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
  journal = {arXiv:1703.06870 [cs]},
  author = {He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
  month = mar,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/arildm/Zotero/storage/Q7QQ5WXE/He et al. - 2017 - Mask R-CNN.pdf;/home/arildm/Zotero/storage/XQ6ZDFKU/1703.html}
}

@article{darknet13,
  title = {Darknet: {{Open Source Neural Networks}} in {{C}}},
  author = {Redmon, Joseph},
  year = {2013}
}

@article{LindstromReviewSituationsAttitudes1991,
  title = {Review of {{Situations}} and {{Attitudes}}},
  volume = {25},
  issn = {0029-4624},
  doi = {10.2307/2215650},
  number = {5},
  journal = {No{\^u}s},
  author = {Lindstr{\"o}m, Sten},
  collaborator = {Barwise, Jon and Perry, John},
  year = {1991},
  pages = {743-770}
}

@misc{SimonDobnikModularmechanisticnetworks2017,
  title = {Modular Mechanistic Networks: {{On}} Bridging Mechanistic and Phenomenological Models with Deep Neural Networks in Natural Language Processing},
  shorttitle = {Modular Mechanistic Networks},
  abstract = {Natural language processing (NLP) can be done using either top-down (theory driven) and bottom-up (data driven) approaches, which we call mechanistic and phenomenological respectively. The approaches are frequently considered to stand in opposition to each other. Examining some recent approaches in deep learning we argue that deep neural networks incorporate both perspectives and, furthermore, that leveraging this aspect of deep learning may help in solving complex problems within language technology, such as modelling language and perception in the domain of spatial cognition.},
  language = {eng},
  publisher = {{CLASP: Centre for Language and Studies in Probability, FLOV, University of Gothenburg, Gothenburg, Sweden}},
  author = {{Simon Dobnik} and {John D. Kelleher}},
  year = {2017},
  keywords = {computational models,data,deep learning,formal models,HUMANIORA|SprÃ¥k och litteratur|JÃ¤mfÃ¶rande sprÃ¥kvetenskap och lingvistik|Lingvistik,HUMANITIES|Languages and Literature|General Language Studies and Linguistics|Linguistics,logic,machine learning,mechanistic models,natural language processing,NATURAL SCIENCES|Computer and Information Science|Language Technology (Computational Linguistics)|Computational linguistics,NATURVETENSKAP|Data- och informationsvetenskap|SprÃ¥kteknologi (sprÃ¥kvetenskaplig databehandling)|Datorlingvistik,neural networks,phenomenological models,rule-based systems,spatial cognition and language}
}

@article{SchlangenResolvingReferencesObjects2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1510.02125},
  primaryClass = {cs},
  title = {Resolving {{References}} to {{Objects}} in {{Photographs}} Using the {{Words}}-{{As}}-{{Classifiers Model}}},
  abstract = {A common use of language is to refer to visually present objects. Modelling it in computers requires modelling the link between language and perception. The "words as classifiers" model of grounded semantics views words as classifiers of perceptual contexts, and composes the meaning of a phrase through composition of the denotations of its component words. It was recently shown to perform well in a game-playing scenario with a small number of object types. We apply it to two large sets of real-world photographs that contain a much larger variety of types and for which referring expressions are available. Using a pre-trained convolutional neural network to extract image features, and augmenting these with in-picture positional information, we show that the model achieves performance competitive with the state of the art in a reference resolution task (given expression, find bounding box of its referent), while, as we argue, being conceptually simpler and more flexible.},
  journal = {arXiv:1510.02125 [cs]},
  author = {Schlangen, David and Zarriess, Sina and Kennington, Casey},
  month = oct,
  year = {2015},
  keywords = {Computer Science - Computation and Language},
  file = {/home/arildm/Zotero/storage/C9A2PQ24/Schlangen et al. - 2015 - Resolving References to Objects in Photographs usi.pdf;/home/arildm/Zotero/storage/JF7EV4PZ/1510.html}
}

@book{Barwisesituationlogic1989,
  address = {Stanford, CA},
  series = {CSLI lecture notes},
  title = {The Situation in Logic},
  isbn = {978-0-937073-33-9 978-0-937073-32-2},
  lccn = {BC57 .B38 1989},
  language = {en},
  number = {no. 17},
  publisher = {{Center for the Study of Language and Information}},
  author = {Barwise, Jon},
  year = {1989},
  keywords = {Context (Linguistics),Language and logic},
  file = {/home/arildm/Zotero/storage/WAUZK8IT/Barwise - 1989 - The situation in logic.pdf}
}

@book{BirdNaturalLanguageProcessing2009,
  edition = {1st},
  title = {Natural {{Language Processing}} with {{Python}}},
  isbn = {978-0-596-51649-9},
  abstract = {This book offers a highly accessible introduction to natural language processing, the field that supports a variety of language technologies, from predictive text and email filtering to automatic summarization and translation. With it, you'll learn how to write Python programs that work with large collections of unstructured text. You'll access richly annotated datasets using a comprehensive range of linguistic data structures, and you'll understand the main algorithms for analyzing the content and structure of written communication. Packed with examples and exercises, Natural Language Processing with Python will help you: Extract information from unstructured text, either to guess the topic or identify "named entities" Analyze linguistic structure in text, including parsing and semantic analysis Access popular linguistic databases, including WordNet and treebanks Integrate techniques drawn from fields as diverse as linguistics and artificial intelligence This book will help you gain practical skills in natural language processing using the Python programming language and the Natural Language Toolkit (NLTK) open source library. If you're interested in developing web applications, analyzing multilingual news sources, or documenting endangered languages -- or if you're simply curious to have a programmer's perspective on how human language works -- you'll find Natural Language Processing with Python both fascinating and immensely useful.},
  publisher = {{O'Reilly Media, Inc.}},
  author = {Bird, Steven and Klein, Ewan and Loper, Edward},
  year = {2009}
}

@book{MontagueFormalPhilosophySelected1974,
  title = {Formal {{Philosophy}}; {{Selected Papers}} of {{Richard Montague}}},
  publisher = {{New Haven: Yale University Press}},
  author = {Montague, Richard},
  year = {1974}
}

@incollection{Leveltperceptuallimitationstalking1984,
  title = {Some Perceptual Limitations on Talking about Space},
  booktitle = {Limits in {{Perception}}},
  publisher = {{VNU Science Press}},
  author = {Levelt, Willem J. M.},
  editor = {{van Doorn et al.}},
  year = {1984},
  pages = {323-358},
  file = {/home/arildm/Zotero/storage/7KPCHMJL/Levelt_Some_Perceptual_1984.pdf}
}

@book{WhiteheadPrincipiamathematicaAlfred2005,
  title = {Principia Mathematica, by {{Alfred North Whitehead}} ... and {{Bertrand Russell}}.},
  author = {Whitehead, Alfred North},
  year = {2005},
  file = {/home/arildm/Zotero/storage/XEUKHV8D/text-idx.html}
}

@book{WhiteheadPrincipiamathematica1910,
  address = {Cambridge [usw.]},
  edition = {2. ed., reprint},
  title = {Principia Mathematica},
  isbn = {978-0-521-06791-1},
  publisher = {{Cambridge Univ. Pr}},
  author = {Whitehead, Alfred North and Russell, Bertrand},
  year = {1910},
  note = {OCLC: 252383138}
}

@book{RantaTypetheoreticalGrammar1995,
  address = {Oxford, New York},
  series = {Indices},
  title = {Type-Theoretical {{Grammar}}},
  isbn = {978-0-19-853857-8},
  abstract = {Constructive type theory was first presented in 1970, by the Swedish logician Per Martin-Lof. It has become one of the main approaches in the foundations of mathematics and computer science. But it has remained relatively unknown among linguists and philosophers, although it provides a comsiderable extension of the concepts and techniques of logic. The book contains an introduction to type theory from the point of view of linguistics and the philosophy of language. Type theory is then applied in the areas of quantification, anaphora, temporal reference, and the structure of text and discourse. In virtue of the type theoretical concepts of proof object and context, various phenomena of dependence and progression in language can be discussed in precise terms, and several well-known problems can be solved.},
  publisher = {{Oxford University Press}},
  author = {Ranta, Aarne},
  month = jan,
  year = {1995},
  file = {/home/arildm/Zotero/storage/CNXAXJLG/type-theoretical-grammar-9780198538578.html}
}

@inproceedings{KohlhaseTypeTheoreticSemanticslDRT1996,
  address = {Amsterdam},
  title = {A {{Type}}-{{Theoretic Semantics}} for $\lambda$-{{DRT}}},
  language = {en},
  booktitle = {Proceedings of the 10th {{Amsterdam Colloquium}}},
  author = {Kohlhase, Michael and Kuschert, Susanna and Pinkal, Manfred},
  editor = {Dekker, P. and Stokhof, M.},
  year = {1996},
  pages = {479--498},
  file = {/home/arildm/Zotero/storage/J8RZR39L/Kohlhase et al. - A Type-Theoretic Semantics for -DRT.pdf}
}


