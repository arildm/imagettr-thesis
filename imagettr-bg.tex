\renewcommand{\sectionautorefname}{Section}
\let\subsectionautorefname\sectionautorefname
\let\subsubsectionautorefname\sectionautorefname
\glsresetall
\section{Background}
\label{sec:background}

This section will highlight some important pieces of the history of past research in relevant fields.

\subsection{Image recognition and object detection}

In image recognition, visual data is analyzed in order to detect and classify objects.
A wide range of models have been developed to solve this task.
Some focus only on the detection of objects \citep{BlaschkoLearningLocalizeObjects2008}, some only on classification \citep[e.g. ResNet,][]{HeDeepResidualLearning2015}, and others attempt to solve the full problem in an integrated fashion \citep{RedmonYouOnlyLook2015,HeMaskRCNN2017}.

Like in machine learning in general, and other kinds of data processing, significant values from the input data (the image) are collected in a process known as feature extraction.
Various types of features exist for image processing.
For one, SIFT is a technique where significant locations of an image are used to extract \textit{keypoints} \citep{LoweObjectrecognitionlocal1999}.
Classification can then be performed by comparing the keypoints of a target image to those in a database.

With neural networks, however, the need for prior feature extraction is generally eliminated \citep{HeDeepResidualLearning2015, HeMaskRCNN2017}.
Neural networks that process image data typically contain convolutional network layers.
Color image data is highly dimensional, typically represented as a 2D matrix of pixels, where each pixel itself specifies a quantity of each of three basic colors.
Convolutional layers are used to divide the image into smaller, overlapping segments, thus capturing locational aspects of the data.
This way, the dimensionality can be reduced to a single, one-dimensional vector.



\subsection{Computational semantics}
\label{sec:compsem}

Semantics is the study of meaning.
Computational semantics is concerned with how to represent meaning digitally, and use it to perform semantic parsing, inference and other tasks \citep{BlackburnComputationalsemantics2003}.

A well-established and largely capable formalism for expressing and operating on propositions is \gls{fol}.
In \gls{fol}, the phrase ``all dogs run'' can be expressed as $\forall x [ \text{dog}(x) \rightarrow \text{run}(x) ]$:
for each individual it holds that if it is a dog then it runs.
\cite{BlackburnComputationalsemantics2003} claimed that \gls{fol} is an adequate semantic representation in a majority of cases, but ``other approaches are both possible and interesting''.

\cite{MontagueFormalPhilosophySelected1974} connected \gls{fol} and lambda calculus with syntactic parsing to obtain semantic parsing.
Thus, a natural-language utterance can be translated to a logical representation.
One method for this is \gls{cfg} with feature structures.
In \gls{cfg}, a set of rules determine how an utterance is parsed into a syntax tree:
A determiner followed by a common noun form a noun phrase (NP $\rightarrow$ Det N), a sentence consists of a noun phrase and a verb phrase (S $\rightarrow$ NP VP), and so on.
Extending a \gls{cfg} framework with feature structures allows constituents to carry additional information, such as semantic representations, which can be combined as defined in the grammar rules.

For a simple example, a noun phrase may carry the term $t_{\text{NP}} = \lambda P \exists x P(x)$, and a verb phrase may carry $t_\text{VP} = \lambda z \ \text{sleep}(z)$.
A sentence rule may combine them as $t_\text{S} \rightarrow t_\text{VP}(t_\text{NP})$.
The result is $t_\text{S} = (\lambda P \exists x P(x)) (\lambda z \ \text{sleep}(z))$ which, after $\beta$-reduction, is equal to $t_\text{S} = \exists x \ \text{sleep}(x)$.
In real applications, the rules for noun and verb phrases are usually more complex in order to cover more complex grammatical constructions.

With recent advancements in computer science, ambitious computational-semantic theories are now in abundance.
As a competitor to formal systems, statistical methods have emerged which do well in various tasks within semantics.
They leverage the performance of modern computers and the large amounts of data that are available as a product of our largely digitalized society.
These data-driven approaches are easily adapted for wide coverage (assuming enough data is available) but they often produce shallow knowledge.
Formal approaches, on the other hand, require more or less precisely crafted rules and formulations, which is time-consuming, but it typically enables the result to be more structured and comprehensive \citep{Dobnik:2017ag}.

\cite{SearleMindsbrainsprograms1980} disputes whether a computer really can \textit{understand} concepts, that is, whether it will be able operate on grounded symbols or just the (arbitrary) symbols themselves.
\cite{HarnadSymbolGroundingProblem1990} names this the \textit{symbol grounding problem}.
\cite{SteelsSymbolGroundingProblem2007} describes experiments where a number of artificial agents participate in a language game, where they make up random words for preset concepts and manage to ``agree'' on which words to use for which concepts.
With the success in these experiments, \citeauthor{SteelsSymbolGroundingProblem2007} concludes that the symbol grounding problem is solved.



\subsection{Perceptual semantics}
% ... in general, and spatial relations in particular

An artificial device perceiving its environment will make internal, symbolic representations of the real world outside \citep{PustejovskyPerceptualsemanticsconstruction1990}.
According to \cite{FregeUberSinnUnd1892}, these symbols will have \textit{sense} as well as \textit{reference}.
A symbol with the sense ``the dog'' may have a certain dog in the environment as reference.
Later, the same symbol and sense may refer to another dog.

By using terms of spatial relations (``left'', ``right'', ``above'', etc.), the location of one object is specified in terms of the location and orientation of another.
Different terminology have been used to refer to the two roles, but we will use \textit{located object} and \textit{reference object} \citep{DobnikModellinglanguageaction2012}.

\cite{Garnhamunifiedtheorymeaning1989} explores terms of spatial relations and claims that there are three types of meanings for each term: basic, deictic and intrinsic.
The basic meaning relative to the speaker and holds for a single object only.
The deictic and intrinsical meanings hold for relations between two objects.
The deictic meaning is relative to the coordinate frame of the speaker, while the intrinsical is relative to that of the reference object.
For someone facing the right side of a car, an object said to be ``to the left of the car'' could be understood to be either near the car's backside (deictic meaning) or its left side (intrinsical meaning).

%The vertical (or gravitational) axis is special, so the terms ``above'' and ``below'' work differently than expected when the related object is, for example, upside down.
%\citeauthor{Garnhamunifiedtheorymeaning1989} uses a rule known as the \textit{framework vertical constraint} to explain this.
%It says that ``above'' and ``below'' must be understood in the 

\cite{LoganComputationalAnalysisApprehension1996} propose \textit{spatial templates} for the classification of spatial relations.
A spatial template is a field of acceptability ratings for a certain spatial relation term.
The center of the field is the location occupied by the reference object and each rating denotes the acceptability of using the term if the located object is at the location of that rating.
The ratings in spatial templates are collected through experiments.

\cite{RegierGroundingspatiallanguage2001a} instead propose a computational classification model known as \textit{attentional vector-sum (AVS)}.
The model considers distance between objects and the fact that they can have different shapes (especially elongated in some direction).
This model is compared to three simpler alternatives in seven experiments, and AVS is found to perform best.

\cite{CoventryInterplayGeometryFunction2001} explores extra-geometric constraints on the meaning of spatial relational terms, especially functional ones.
The functional relation between objects is significant for the acceptability of terms of spatial relations.
For example, an umbrella may be well said to be \textit{above} a man, but less clearly \textit{over} him, if it does not protect him from rain falling sideways in hard wind \citep{CoventryInterplayGeometryFunction2001}.

%"say something about recognitising objects as well and grounding in general. So I would add discussions of Roy" SD



\subsection{\Gls{ttr}}
\label{sec:ttnlp}

Type theory is a logic system developed by \cite{WhiteheadPrincipiamathematica1910}, \cite{church40}, \cite{martinlof84} among others \citep{CoquandTypeTheory2015}.
The theory revolves around the concept that any object belongs to a type.
The judgement that an object $a$ belongs to a type $T$ is written $a:T$.
Functions are restricted to certain types, which allows more specificity in how they can be applied.
For example, the factorial function $f_!$ may be defined over natural numbers by typing it as $f_! :\mathbb{N} \rightarrow \mathbb{N}$.
%Several different type theories have been created, of which Church's \textit{simply typed \textlambda-calculus} \cite{church40} and Martin-LÃ¶f's \textit{intuitionistic type theory} \citep{martinlof84} are some prominent examples.

\cite{RantaTypetheoreticalGrammar1995} uses type theory to drive a method of syntactic parsing.
At a glance, consider how the type-theoretical judgements $\text{``the''} : \mathit{Det}$, $\text{``door''} : \mathit{N}$ and $f_\text{DetN}:\mathit{Det}\rightarrow\mathit{N}\rightarrow\mathit{NP}$ dictate that $f_\text{DetN}(\text{``the''}, \text{``door''})$ is an object of the type $\mathit{NP}$.

Another example of type theory in \gls{nlp} is \cite{KohlhaseTypeTheoreticSemanticslDRT1996}, which extends Discourse Representation Theory (DRT) with elements from type theory in order to provide compositionality.

%[a hint of what tt provides]

% Introducing TTR

\glsreset{ttr}
\Gls{ttr} \citep{CooperRecordsRecordTypes2005} combines several theories from logic, semantics and linguistics in a single type-theoretic framework.
It employs \textit{records}, objects which themselves are structured compositions of other objects;
and accordingly, \textit{record types} which are structures of other types.
More details on the features of \gls{ttr} are given in the overview in \autoref{sec:ttr}.

\gls{ttr} has primarily been used to power various accounts of \gls{nlp}, for example
syntax \citep{CooperAustiniantruthattitudes2005, CooperRecordsRecordTypes2005, CooperTypetheorysemantics2012, CooperTypetheorylanguage2016},
dialogue \citep{Larssonformalviewcorrective2009, LarssonDialoguesHaveContent2011, CooperTypetheorylanguage2016},
situated agents \citep{DobnikModellinglanguageaction2012, ttrspat,lspc} and
spoken language \citep{CooperTypetheorylanguage2016}.

% Usage of TTR in NLP, semantics, perceptual semantics

\cite{DobnikModellinglanguageaction2012} presents how \gls{ttr} can be used to model a situated conversational agent.
The agent moves around in a point-space world.
Objects, detected as sub-point-spaces are recognized, as are (geometric and extra-geometric) spatial relations between them.
The work is followed up by \cite{ttrspat} and \cite{lspc}, which model similar situated agents.

%\cite{LarssonFormalsemanticsperceptual2015}



\subsubsection{Overview of \gls{ttr}}
\label{sec:ttr}

This section is an overview of \gls{ttr} based on \cite{CooperTypetheorysemantics2012} and \cite{CooperTypetheorylanguage2016}.

In \gls{ttr} there are two kinds of entities: types and objects.
Each type is associated with a set of objects which are of that type, or in other words are \textbf{witnesses} of that type.
The judgement that the object $a$ is a witness of the type $T$ is written $a : T$.
For example, $34 : Int$ means that $34$ is of the type $Int$.

A \textbf{record type} is a set of fields, each field carrying a label and a type.
In the record type $\rec{\text{person} &:& Ind \\ \text{age} &:& Int}$ there is a field labeled `person' of type $Ind$, and one labeled `age' of the type $Int$.

The witnesses of record types are \textbf{records}.
A record is also a set of fields with labels, but instead of a type, a label is associated with an object.
A record $r$ is a witness of a record type $T$, \textit{if and only if} every field in the record type has a matching field in the record, that is, the labels are the same and the object in the record field is a witness of the type in the record type.
%The record may have additional fields that are not in the record type; this does not interfere with type judgement.
The record $\rec{\text{person} &=& a_{12} \\ \text{age} &=& 28}$ is a witness of the record type just mentioned, provided $a_{12} : Ind$ and $28 : Int$.

% Intensional/extensional? (TTL p. 6)

A type $T_{sub}$ is a \textbf{subtype} of another type $T_{super}$, written $T_{sub} \sqsubseteq T_{super}$, if any witness of the subtype is necessarily also a witness of the supertype.
For example, $Int \sqsubseteq Number$.
%TODO "necessarily"?
In the case of record types, this means that a record type $T_{sub}$ is a subtype of a record type $T_{super}$ if and only if every field in $T_{super}$ is present also in $T_{sub}$ (allowing that a type in a $T_{sub}$ field is itself a subtype of the type in the corresponding $T_{super}$ field).
For example, $\rec{\text{x} &:& Int} \sqsubseteq \rec{\text{x} &:& Number}$, provided $Int \sqsubseteq Number$.

Relationships between objects can be modeled using \textbf{ptypes}, complex types constructed from predicates and objects.
For instance, the type $\text{hug}(a,b)$ is constructed from the predicate 'hug' with the objects $a$ in the first place and $b$ in the second.
The situation that $a$ is hugging $b$ is true if there exists a witness of $\text{hug}(a,b)$.
In a record type, a ptype typically uses the labels of other fields for its arguments:
In $\rec{\text{x} &:& Ind \\ \text{c} &:& \text{green}(\text{x})}$, the field `c' is \textit{dependent} on the field `x'.

A type can be restricted to only have a single witness:
If $T$ is a type and $a:T$, then $T_a$ is a \textbf{singleton type} having $a$ as its only witness.
There is an alternative notation for singleton types used in record types:
The record type $\rec{\text{x} &:& Ind_a}$ can also be written $\rec{\text{x}=a &:& Ind}$, and is restricted to the record $\rec{\text{x} &:& a}$.
A singleton-typed field notated this way is known as a \textbf{manifest field}.
Any number of fields in a record type may be manifest fields.

A function from $T_{dom}$ to $T_{rng}$ is a witness of the \textbf{function type} $T_{dom} \rightarrow T_{rng}$.
For instance, if $f = \lambda x : Ind\ .\ \text{blue}(x)$, then $f : Ind \rightarrow Type$.

A list of objects of type $T$ is a witness of the \textbf{list type} $[T]$:
If $L$ is a list and $\forall a \in L, a : T$, then $L : [T]$.
The list containing $a$, $b$ and $c$ can be written $[a, b, c]$.
(This notation may not have been proposed in past \gls{ttr} literature.)

Types themselves may be the witnesses of other types.
$Type$ is the type of all types, so if $T$ is a type, then $T : Type$.
(The controversial consequence that $Type : Type$ is handled in \citet[section 2.7]{CooperTypetheorysemantics2012}.)
Similarly, $RecType$ is the type of all record types.
% PType, the type of ptypes?

The \textbf{relabeling} $\eta$ of a record type $T$ is a set of tuples where the first element is a label in $T$ and the second is another, new label.
$T_\eta$ is another record type, similar to $T$ but where the first item in each element of $\eta$ has been replaced with the second item.
So, if $T = \rec{\text{x} &:& T'}$ and $\eta = [\langle \text{x}, \text{y}\rangle]$, then $T_\eta = \rec{\text{y} &:& T'}$.

\textbf{Flattening} transforms a nested record type into an non-nested record type.
In a nested record type such as $T = \rec{x &:& \rec{y &:& T_1} \\ z &:& T_2}$, a \textbf{path} of labels from consecutive levels can be used to address a nested field, thus $x.y$ refers to the field with the type $T_1$.
The flattened type $\varphi(T)$ contains every field in the first level, and the paths from the nested type are used as labels:
$\varphi(T) = \rec{x.y &:& T_1 \\ z &:& T_2}$

The \textbf{meet} of two types $T_1 \wedge T_2$ is a new type whose witnesses are those that are witnesses both of $T_1$ and $T_2$ (an intersection of the sets of witnesses).
The \textbf{join} $T_1 \vee T_2$ is a type whose witnesses are those that are witnesses either of $T_1$ or of $T_2$, or both (a union).

The \textbf{merge} of two types $T_1 \ttrmerge T_2$ is a more complicated operation.
If $T_1$ and $T_2$ are record types, their fields are added together to a new record type;
if any label occurs in both types, so $T_1$ has $\langle \ell, T'_1\rangle$ and $T_2$ has $\langle \ell, T'_2\rangle$, a field with that label is added, which has the merge of the two field types, $\langle \ell, T'_1 \ttrmerge T'_2\rangle$.
If any of $T_1$ and $T_2$ is not a record type, then $T_1 \ttrmerge T_2 = T_1 \wedge T_2$.



\subsection{\Acrfull{vqa}}

\cite{AgrawalVQAVisualQuestion2015} suggest \gls{vqa} as a challenge for multi-modal semantic systems.
A \gls{vqa} system is presented an image and a natural-language question about the image, and is expected to produce a natural-language answer.
The initiative includes datasets and a series of annual competitions since 2016.

A neural-network approach to question answering tasks in general is proposed by \cite{AndreasLearningComposeNeural2016}, where multiple neural-network modules are assembled like constituents in a syntax tree.
For example, for the \gls{vqa} question ``What color is the bird?'', a network that locates objects of a given class is connected to one which classifies the color at the indicated location.
The method of composing the various modules is trained jointly with the module networks themselves.

%\cite{SchlangenResolvingReferencesObjects2015}

%[example questions/answers]
