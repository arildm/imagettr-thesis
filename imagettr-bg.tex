\def\equationautorefname{equation}
\let\subsectionautorefname\sectionautorefname
\let\subsubsectionautorefname\sectionautorefname
\glsresetall
\section{Background}
\label{sec:background}

This section will highlight some important pieces of the history of past research in relevant fields.

\subsection{Computational semantics}

Semantics is the study of meaning.
Computational semantics is concerned with how to represent meaning digitally, and then use it to perform semantic parsing, inference and other tasks \citep{BlackburnComputationalsemantics2003}.
A well-established and largely capable formalism for expressing and operating on propositions is \gls{fol}.
\cite{MontagueFormalPhilosophySelected1974} used \gls{fol} with syntactic parsing for semantic parsing.
Thus, a natural-language utterance can be translated to a logical representation.

With recent advancements in computer science, ambitious computational-semantic theories are now in abundance.
As a competitor to formal systems, statistical methods have emerged which do well in various tasks within semantics.
They utilize the performance of modern computers and leverage the large amounts of data that are available as a product of our largely digitalized society.
Data-driven approaches are easily adapted for wide coverage (assuming enough data is available) but they often produce shallow knowledge.
Formal approaches, on the other hand, require more or less precisely crafted rules and formulations, which is time-consuming, but it typically enables the result to be more structured and comprehensive \citep{Dobnik:2017ag}.

\cite{BlackburnComputationalsemantics2003} claimed (at that time) that \gls{fol} is an adequate semantic representation in a majority of cases, but ``other approaches are both possible and interesting''.

\cite{SearleMindsbrainsprograms1980} disputes whether a computer really can \textit{understand} concepts, that is, whether it will be able operate on grounded symbols or just the (arbitrary) symbols themselves.
\cite{HarnadSymbolGroundingProblem1990} names this the \textit{symbol grounding problem}.
\cite{SteelsSymbolGroundingProblem2007} describes experiments where a number of agents participate in a language game, where they make up random words for preset concepts and manage to ``agree'' on which words to use for which concepts.
With the success in these experiments, \citeauthor{SteelsSymbolGroundingProblem2007} concludes that the symbol grounding problem is solved.



\subsection{Perceptual semantics}
% ... in general, and spatial relations in particular

An artificial device perceiving its environment will make internal, symbolic representations of the real world outside \citep{PustejovskyPerceptualsemanticsconstruction1990}.
According to \cite{FregeUberSinnUnd1892}, these symbols will have \textit{sense} as well as \textit{reference}.
A symbol with the sense ``the dog'' may have a certain dog in the environment as reference.
Later, the same symbol and sense may refer to another dog.

By using terms of spatial relations (``left'', ``right'', ``above'', etc.), the location of one object is specified in terms of the location and orientation of another.
Different terminology have been used to refer to the two roles, but we will use \textit{located object} and \textit{reference object} \citep{DobnikModellinglanguageaction2012}.

\cite{Garnhamunifiedtheorymeaning1989} explores terms of spatial relations and claims that there are three types of meanings for each term: basic, deictic and intrinsic.
The deictic and intrinsical meanings hold for relations between two objects.
The deictic meaning is relative to the coordinate frame of the speaker, while the intrinsical is relative to that of the reference object.
The basic meaning, introduced by \citeauthor{Garnhamunifiedtheorymeaning1989}, is also relative to the speaker, but holds for a single object only.

%The vertical (or gravitational) axis is special, so the terms ``above'' and ``below'' work differently than expected when the related object is, for example, upside down.
%\citeauthor{Garnhamunifiedtheorymeaning1989} uses a rule known as the \textit{framework vertical constraint} to explain this.
%It says that ``above'' and ``below'' must be understood in the 

\cite{LoganComputationalAnalysisApprehension1996} propose \textit{spatial templates} for the classification of spatial relations.
A spatial template is a field of acceptability ratings for a certain spatial relation term.
The center of the field is the location occupied by the reference object and each rating denotes the acceptability of using the term if the located object is at the location of that rating.
The ratings in spatial templates are collected through experiments.

\cite{RegierGroundingspatiallanguage2001a} instead propose a computational classification model known as \textit{attentional vector-sum (AVS)}.
The model considers distance between objects and the fact that they can have different shapes (especially elongated in some direction).
This model is compared to three simpler alternatives in seven experiments, and AVS is found to perform best.

\cite{CoventryInterplayGeometryFunction2001} explores extra-geometric constraints on the meaning of spatial relational terms, especially functional ones.
The functional relation between objects is significant for the acceptability of terms of spatial relations.
For example, an umbrella may be well said to be \textit{above} a man, but less clearly \textit{over} him, if it does not protect him from rain falling sideways in hard wind \citep{CoventryInterplayGeometryFunction2001}.

%"say something about recognitising objects as well and grounding in general. So I would add discussions of Roy" SD



\subsection{\Acrfull{ttr}}
\label{sec:ttnlp}

Type theory is a logic system developed by \cite{WhiteheadPrincipiamathematica1910}, \cite{church40}, \cite{martinlof84} among others \citep{CoquandTypeTheory2015}.
The theory revolves around the concept that any object belongs to a type.
The judgement that an object $a$ belongs to a type $T$ is written $a:T$.
Functions are restricted to certain types, which allows more specificity in how they can be applied.
For example, the factorial function $f_!$ may be defined over natural numbers by typing it as $f_! :\mathbb{N} \rightarrow \mathbb{N}$.
%Several different type theories have been created, of which Church's \textit{simply typed \textlambda-calculus} \cite{church40} and Martin-LÃ¶f's \textit{intuitionistic type theory} \citep{martinlof84} are some prominent examples.

\cite{RantaTypetheoreticalGrammar1995} uses type theory to drive a method of syntactic parsing.
At a glance, consider how the type-theoretical judgements $\text{``the''} : \mathit{Det}$, $\text{``door''} : \mathit{N}$ and $f_\text{DetN}:\mathit{Det}\rightarrow\mathit{N}\rightarrow\mathit{NP}$ dictate that $f_\text{DetN}(\text{``the''}, \text{``door''})$ is an object of the type $\mathit{NP}$.

Another example of type theory in \gls{nlp} is \cite{KohlhaseTypeTheoreticSemanticslDRT1996}, which extends Discourse Representation Theory (DRT) with elements from type theory in order to provide compositionality.

%[a hint of what tt provides]

% Introducing TTR

\glsreset{ttr}
\Gls{ttr} \citep{CooperRecordsRecordTypes2005} combines several theories from logic, semantics and linguistics in a single type-theoretic framework.
It employs \textit{records}, objects which themselves are structured compositions of other objects;
and accordingly, \textit{record types} which are structures of other types.
More details on the features of \gls{ttr} are given in the overview in \autoref{sec:ttr}.

\gls{ttr} has primarily been used to power various accounts of \gls{nlp}, for example
syntax \citep{CooperAustiniantruthattitudes2005, CooperRecordsRecordTypes2005, CooperTypetheorysemantics2012, CooperTypetheorylanguage2016},
dialogue \citep{Larssonformalviewcorrective2009, LarssonDialoguesHaveContent2011, CooperTypetheorylanguage2016},
situated agents \citep{DobnikModellinglanguageaction2012, ttrspat,lspc} and
spoken language \citep{CooperTypetheorylanguage2016}.

% Usage of TTR in NLP, semantics, perceptual semantics

\cite{DobnikModellinglanguageaction2012} presents how \gls{ttr} can be used to model a situated conversational agent.
The agent moves around in a point-space world.
Objects, detected as sub point-spaces are recognized, as are (geometric and extra-geometric) spatial relations between them.
The work is followed up by \cite{ttrspat} and \cite{lspc}, which model similar situated agents.

%\cite{LarssonFormalsemanticsperceptual2015}



\subsubsection{Overview of \gls{ttr}}
\label{sec:ttr}

This section is a brief and rather informal overview of \gls{ttr}.
See \cite{CooperTypetheorysemantics2012} for a slightly more verbose presentation with formal definitions, or \cite{CooperTypetheorylanguage2016} for an updated formal definition as well as a more extensively worded informal presentation.
Some concepts are missing in the former but included in the latter, including list types.

In \gls{ttr} there are two kinds of entities: types and objects.
Each type is associated with a set of objects, which are of the type, or are \textbf{witnesses} of the type.
The judgement that the object $a$ is a witness of the type $T$ is written $a : T$.
For example, $34 : Int$ means that $34$ is of the type $Int$.

A \textbf{record type} is a set of fields, each field carrying a label and a type.
In the record type $\left[ \begin{array}{rcl} \text{person} &:& Ind \\ \text{age} &:& Int \end{array} \right]$ there is a field labeled $person$ of type $Ind$, and one labeled $age$ of the type $Int$.

The witnesses of record types are \textbf{records}.
A record is also a set of fields with labels, but instead of a type, a label is associated with an object.
A record $r$ is a witness of a record type $T$, \textit{if and only if} every field in the record type has a matching field in the record, that is, the labels are the same and the object in the record field is a witness of the type in the record type.
%The record may have additional fields that are not in the record type; this does not interfere with type judgement.
The record $\left[ \begin{array}{rcl} \text{person} &=& a_{12} \\ \text{age} &=& 28 \end{array} \right]$ is a witness of the record type just mentioned, provided $a_{12} : Ind$ and $28 : Int$.

% Intensional/extensional? (TTL p. 6)

A type $T_{sub}$ is a \textbf{subtype} of another type $T_{super}$, written $T_{sub} \sqsubseteq T_{super}$, if any witness of the subtype is necessarily also a witness of the supertype.
For example, $Int \sqsubseteq Number$.
%TODO "necessarily"?
In the case of record types, this means that a record type $T_{sub}$ is a subtype of a record type $T_{super}$ if and only if every field in $T_{super}$ is present also in $T_{sub}$ (allowing that a type in a $T_{sub}$ field is itself a subtype of the type in the corresponding $T_{super}$ field).
For example, $\left[ \begin{array}{rcl} x &:& Int \end{array} \right] \sqsubseteq \left[ \begin{array}{rcl} x &:& Number \end{array} \right]$, provided $Int \sqsubseteq Number$.

Relationships between objects can be modeled using \textbf{ptypes}, complex types constructed from predicates and objects.
For instance, the type $\text{hug}(a,b)$ is constructed from the predicate 'hug' with the objects $a$ in the first place and $b$ in the second.
The situation that $a$ is hugging $b$ is true if there exists a witness of $\text{hug}(a,b)$.
[What can such a witness be like?]

A type can be restricted to only have a single witness:
If $T$ is a type and $a:T$, then $T_a$ is a \textbf{singleton type} having $a$ as its only witness.
There is an alternative notation for singleton types used in record types:
The record type $[x:Ind_a]$ can also be written $[x=a:Ind]$, and is restricted to the record $[x=a]$.
A singleton-typed field notated this way is known as a \textbf{manifest field}.
Any number of fields in a record type may be manifest fields.

[discuss] A function from $T_{dom}$ to $T_{rng}$ is a witness of the \textbf{function type} $(T_{dom} \rightarrow T_{rng})$.
For instance, if $f = \lambda x : Ind\ .\ \text{blue}(x)$, then $f : (Ind \rightarrow Type)$.

A list of objects of type $T$ is a witness of the \textbf{list type} $[T]$:
If $L$ is a list and $\forall a \in L, a : T$, then $L : [T]$.
The list containing $a$, $b$ and $c$ can be written $[a, b, c]$.
(This notation may not have been proposed in past \gls{ttr} literature.)

[discuss] Types themselves may be the witnesses of other types.
$Type$ is the type of all types, so if $T$ is a type, then $T : Type$.
(The controversial consequence that $Type : Type$ is handled in \citet[section 2.7]{CooperTypetheorysemantics2012}.)
Similarly, $RecType$ is the type of all record types.
% PType, the type of ptypes?

[TODO Relabel, $\eta$, flatten]

[TODO Merge, join, meet]



\subsubsection{\gls{ttr} programming: PyTTR}
\label{sec:pyttr}

\cite{pyttr} provides a Python implementation of \gls{ttr} known as PyTTR.
It supports the modeling of \gls{ttr} types and operations such as judgement and type checking.
As a Python library it also enables other features and peripheral procedures to be written in Python.

PyTTR allows, in turn, the implementation of \gls{ttr} models.
By implementing a theoretical model as a computer program, it can ``come alive'' and be tested on real problems and data.
When implemented, the model can be evaluated and compared in practical settings it to other models.

%[What has been written in PyTTR so far? nu, animat]



\subsection{Image recognition and object detection}

In image recognition, visual data is analyzed in order to detect and classify objects.
A wide range of models have been developed to solve this task.
Some focus only on the detection of objects \citep{BlaschkoLearningLocalizeObjects2008}, some only on classification \citep[ResNet,][]{HeDeepResidualLearning2015}, and others attempt to solve the full problem in an integrated fashion \citep{RedmonYouOnlyLook2015,HeMaskRCNN2017}.

There are different strategies for encoding the image into features.
For one, SIFT is a technique where significant locations of an image are used to extract \textit{keypoints} \citep{LoweObjectrecognitionlocal1999}.
Classification can then be performed by comparing the keypoints of a target image to those in a database.

With \textit{convolutional neural networks}, however, the need for prior feature extraction is generally eliminated \citep{HeDeepResidualLearning2015, HeMaskRCNN2017}.
Color image data is highly dimensional: it is typically represented digitally as a 2D matrix of pixels, where each pixel itself specifies a quantity of each of three basic colors.
Convolutional layers reduce the dimensionality so that an image can be encoded in a single (one-dimensional) vector.
The full image is often first divided into same-size overlapping segments, thus capturing locational aspects of the data.



\subsection{\Acrfull{vqa}}

\cite{AgrawalVQAVisualQuestion2015} suggest \gls{vqa} as a challenge for multi-modal semantic systems.
A \gls{vqa} system is presented an image and a natural-language question about the image, and is expected to produce a natural-language answer.
The initiative includes datasets and a series of annual competitions since 2016.

A neural-network approach to question answering tasks in general is proposed by \cite{AndreasLearningComposeNeural2016}, where multiple neural-network modules are assembled like constituents in a syntax tree.
For example, for the \gls{vqa} question \textit{What color is the bird?}, a network that detects objects of a given class is connected to one which classifies the color at the indicated location.
The method of assembly is trained jointly with the module networks.

%\cite{SchlangenResolvingReferencesObjects2015}

%[example questions/answers]
