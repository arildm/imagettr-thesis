\section{Discussion}
\label{sec:discussion}



\subsection{Model limitations}

As discussed in \autoref{sec:method-spatrel}, our treatment of spatial relations excludes the intrinsical meaning and functional aspects, in favor of model simplicity and easy implementation.
Both features are treated in terms of \gls{ttr} by \cite{ttrspat}.
The former requires a notion of the orientation of reference objects.
Assuming an object classifier with this capability were available, implementing intrinsical spatial relations would not be a large step from the present model.
Support for the functional aspect of spatial relations requires two things.
Firstly, classifiers for functional relations (such as $\text{protects(o}_3\text{.a, o}_1\text{.a, o}_2\text{.a)}$ in \cite{ttrspat}, for an umbrella protecting a man from rain).
Secondly, prediction from a set of functional relations to spatial classifiers that are sensitive to those functional relations.
The second is needed to activate the appropriate spatial relation term depending on which functional relations are true according to the classifiers in the first.

The model is restricted to a limited type of polar questions, far from the range of question types in the \gls{vqa} dataset \citep{AgrawalVQAVisualQuestion2015}.
Vast extensions to the language parsing, the perceptual classification and the comparation of question and scene are possible and required in order to answer more advanced questions.
Some examples are:
further property classification (``Is the dog brown?''),
wh-questions (``What is to the left of the car?''),
quantities (``How many flowers are there?''),
inference (``Does this person have 20/20 vision?'', for an image where a person is wearing glasses).



\subsection{TTR coverage}



\subsection{VQA performance}



\subsection{Comparison to different approaches}
% a) VQA models, b) other perception/language models



