\documentclass[11pt, a4paper]{article}

\usepackage{mlt-thesis-2015}

\usepackage[english]{babel}
\usepackage[hidelinks]{hyperref}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{comment}
\usepackage{caption}

\usepackage[acronym]{glossaries}
\glsdisablehyper

\title{Visual question answering with type theory}
\subtitle{Using \gls{ttr} to model visual perception and language}
\author{Arild Matsson}
\date{}

\newacronym{hmm}{HMM}{Hidden Markov Model}
\newacronym{lstm}{LSTM}{Long Short-Term Memory}
\newacronym{cnn}{CNN}{convolutional neural network}
\newacronym{ttr}{TTR}{type theory with records}
\newacronym{nlg}{NLG}{natural language generation}
\newacronym{vqa}{VQA}{visual question answering}
\newacronym{yolo}{YOLO}{You only look once}
\newacronym{nlp}{NLP}{natural language processing}

\begin{document}

%% ============================================================================
%% Title page
%% ============================================================================
\begin{titlepage}

\maketitle

\vfill

\begingroup
\renewcommand*{\arraystretch}{1.2}
\begin{tabular}{l@{\hskip 20mm}l}
\hline
Master's Thesis: & 30 credits\\
Programme: & Masterâ€™s Programme in Language Technology\\
Level: & Advanced level \\
Semester and year: & Spring, 2018\\
Supervisors & Simon Dobnik and Staffan Larsson\\
Examiner & (name of the examiner)\\
Keywords & type theory, image recognition, object recognition, visual question answering
\end{tabular}
\endgroup

\thispagestyle{empty}
\end{titlepage}

%% ============================================================================
%% Abstract
%% ============================================================================
\newpage
\singlespacing
\section*{Abstract}

%\begin{abstract}
I'm connecting an off-the-shelf image recognition system to PyTTR, a Python implementation of TTR.
This enables representing concepts in the image as TTR types.
Then I'm implementing a \gls{vqa} task.
Questions are represented in TTR through really simple text parsing, and are answered using the representation of the image recognition results.
This shows that \gls{ttr} is helpful for representing visual perception, semantics and cognition.
\glsresetall
%\end{abstract}

\thispagestyle{empty}

%% ============================================================================
%% Preface
%% ============================================================================
\newpage
\section*{Preface}

Acknowledgements, etc.

\thispagestyle{empty}

%% ============================================================================
%% Contents
%% ============================================================================
\newpage

\begin{spacing}{0.0}
\tableofcontents
\end{spacing}

\thispagestyle{empty}

%% ============================================================================
%% Introduction
%% ============================================================================
\newpage
\setcounter{page}{1}

\section{Introduction}
\label{sec:intro}

(Just copied from the thesis proposal for now...)

This project will explore how \gls{ttr} can be used with image recognition tasks such as question answering and description generation.
How can off-the-shelf image recognition software output be expressed in \gls{ttr}, and what can we do with it?

\begin{itemize}
\item Express image classification results in \gls{ttr}
\item Detect and recognize multiple objects in an image, as well as relationships between them
\begin{itemize}
\item Spatial, geometric relationships such as "above" and "to the left of"
\item Interaction such as "riding" or "holding"
\end{itemize}
\item Question anwering: parsing questions or statements into \gls{ttr} and judging their validity in relation to an image
\item \gls{nlg} of image descriptions
\end{itemize}

\noindent
Development should focus on utilizing and possibly extending PyTTR, a Python implementation of \gls{ttr} \citep{pyttr}. Image recognition and natural language parsing/generation should use existing solutions as far as possible.

\section{Background}

\subsection{Type theory}

\subsubsection{Type theory, semantics and \gls{nlp}}

\glsreset{ttr}
\subsection{\Gls{ttr}}

\gls{ttr} is a formal framework for semantics \citep{CooperRecordsRecordTypes2005}.
It has been employed to model natural language in the context of dialogue, situated agents and spoken language.

\subsubsection{Applications of \gls{ttr}}

\cite{DobnikModellinglanguageaction2012} model a robot that would move around and use laser range scanner or similar to collect points in space, group them into objects and detect spatial relations between them.
\Gls{ttr} is used throughout the model, accounting for perception and cognition.

\subsubsection{PyTTR}

\subsection{Visual object recognition}

\cite{Detectron2018}

\subsubsection{\Gls{vqa}}

\subsubsection{YOLO object recognition model}

You only look once (YOLO) \citep{RedmonYouOnlyLook2015} is a neural network model that simultaneously predicts bounding boxes around objects and classifies the contained objects.

\section{\Gls{vqa} using \gls{ttr}}

\subsection{Modelling recognised objects}

The off-the-shelf image recognition model is applied to an image.
The output is a set of objects containing bounding box coordinates, a label and a confidence score.
These objects are considered as input to the proposed model.

\begin{equation}\label{eq:seg}
Segment = \left[\begin{array}{rcl}
\text{cy} &:& Int\\
\text{cx} &:& Int\\
\text{w} &:& Int\\
\text{h} &:& Int
\end{array}\right]\end{equation}

\begin{equation}\label{eq:ppty}
Ppty = (Ind\rightarrow Type)\end{equation}

\subsubsection{Alt 1: DetectedInd}

While the confidence score is discarded, the position and the label of each object are used to create a record representing a detected individual.
The type of this record, $DetectedInd$, is defined in \autoref{eq:detind} and an example record is given in \autoref{eq:detindrec}.
The $seg$ field is of the type $Segment$, which is defined in \autoref{eq:seg}, and models the position of this object in the image.
The $pfun$ field is of the type $Ppty$, defined in \autoref{eq:ppty} as a function type from $Ind$ to a ptype.
It describes the classification result, a property of the detected object.
Up to this point, the procedure is heavily inspired by Section~5.1 in \cite{DobnikInterfacinglanguagespatial2017}.

\begin{equation}\label{eq:detind}
DetectedInd = \left[\begin{array}{rcl}
\text{seg} &:& Segment\\
\text{pfun} &:& Ppty \\
\text{ind} &:& Ind
\end{array}\right]\end{equation}

\begin{equation}\label{eq:detindrec}
\left[\begin{array}{rcl}
\text{seg} &=& \left[\begin{array}{rcl}
\text{cx} &=& 138\\
\text{w} &=& 276\\
\text{cy} &=& 654\\
\text{h} &=& 809
\end{array}\right]\\
\text{pfun} &=& \lambda v:Ind\ .\ \text{person}(v)\\
\text{ind} &=& a_{10}
\end{array}\right] : DetectedInd\end{equation}

When the set of detected objects is modeled as a set of $DetectedInd$ records, it doesn't say a lot in itself.
What matters is what type they might have.
Types are generated by applying the function $WhatIs$, described in \autoref{eq:whatis}.
The generated types are situations that have been observed.

\begin{equation}\label{eq:whatis}
WhatIs = \lambda r:DetectedInd\ .\ \left[\begin{array}{rcl}
\text{a} &:& Ind\\
\text{c} &:& \langle r.\text{pfun}, [a]\rangle
\end{array}\right]\end{equation}

A simple list of the situation types does not provide power enough (??).
They are better combined into one record type that solely describes a situation that includes all the observed situations. Combination is most suitably done with a two-step combine-and-flatten method.

In the \textit{combine} step, each situation type is inserted into the next, under a label $prev$.
The function given in \autoref{eq:comb} is recursively applied to the individual situation types.
An example combination result is shown in \autoref{eq:combrt}.
The resulting type contains all parts under different levels, and is cumbersome to work with.

In the next step, the type is \textit{flattened} into a one-level record type as exemplified in \autoref{eq:combrtflat}.
The $prev$ labels remain as parts of other labels although they are nonsensical in the current context.
They can however be substituted through \textit{relabeling} as needed.

\begin{equation}\label{eq:comb}
Combine = \lambda T_1 : RecType \ .\ \lambda T_2 : RecType \ .\
[\begin{array}{rcl} \text{prev} &:& T_1 \end{array}] \cdot\wedge T_2
% @todo Fix cdot+wedge
\end{equation}

\begin{equation}\label{eq:combrt}
\left[\begin{array}{rcl}
\text{prev} &:& \left[\begin{array}{rcl}
	\text{prev} &:& \left[\begin{array}{rcl}
		\text{a} &:& Ind\\
		\text{c} &:& \text{dog}(a)
		\end{array}\right]\\
	\text{a} &:& Ind\\
	\text{c} &:& \text{car}(a)
	\end{array}\right]\\
\text{a} &:& Ind\\
\text{c} &:& \text{person}(a)
\end{array}\right]\end{equation}

\begin{equation}\label{eq:combrtflat}
\left[\begin{array}{rcl}
\text{prev.prev.a} &:& Ind\\
\text{prev.prev.c} &:& \text{dog}(prev.prev.a)\\
\text{prev.a} &:& Ind\\
\text{prev.c} &:& \text{car}(prev.a)\\
\text{a} &:& Ind\\
\text{c} &:& \text{person}(a)\\
\end{array}\right]\end{equation}

\subsubsection{Alt 2: IndFun like LSPC}

The function given in \autoref{eq:indfun} can be applied to an $ImageDetection$ record to return a situation type describing an individual object being observed at some location.

\begin{equation}\label{eq:indfun}
\lambda r : ImageDetection\ .\ 
\left[\begin{array}{rcl}
\text{a} = Ind &:& Ty\\
\text{c} &:& r.\text{pfun}(\text{a})\\
\text{loc} &:& \text{location}(\text{a}, r.\text{seg})\\
\end{array}\right]\end{equation}

An example of such a type is given in \autoref{eq:ind1}.
(The image is an object of this type??)

\begin{equation}\label{eq:ind1}
\left[\begin{array}{rcl}
\text{a} &:& Ind\\
\text{c} &:& \text{person}(a)\\
\text{loc} &:& \text{location}(a, \left[\begin{array}{rcl}
\text{cy} &=& 654\\
\text{h} &=& 809\\
\text{cx} &=& 138\\
\text{w} &=& 276
\end{array}\right])\\
\end{array}\right]\end{equation}

Applying the function to each record returned by the object recognizer, and then combine-and-flattening the resulting situation types, may result in a type like the one in \autoref{eq:indmul}.

\begin{equation}\label{eq:indmul}
\left[\begin{array}{rcl}
\text{prev.prev.a} &:& Ind\\
\text{prev.prev.c} &:& \text{sofa}(\text{prev.prev.a})\\
\text{prev.prev.loc} &:& \text{location}(\text{prev.prev.a}, \left[\begin{array}{rcl}
\text{cy} &=& 677\\
\text{h} &=& 803\\
\text{cx} &=& 486\\
\text{w} &=& 957
\end{array}\right])\\
\text{prev.a} &:& Ind\\
\text{prev.c} &:& \text{cell phone}(\text{prev.a})\\
\text{prev.loc} &:& \text{location}(\text{prev.a}, \left[\begin{array}{rcl}
\text{cy} &=& 588\\
\text{h} &=& 423\\
\text{cx} &=& 93\\
\text{w} &=& 187
\end{array}\right])\\
\text{a} &:& Ind\\
\text{c} &:& \text{clock}(\text{a})\\
\text{loc} &:& \text{location}(\text{a}, \left[\begin{array}{rcl}
\text{cy} &=& 544\\
\text{h} &=& 107\\
\text{cx} &=& 44\\
\text{w} &=& 71
\end{array}\right])\\
\end{array}\right]\end{equation}

\subsection{Spatial relationships}

From the information modeled above, how can we see what relationships occur between individuals?


\subsection{Parsing}

A dog is to the left of a car
\begin{equation}\left[\begin{array}{rcl}
\text{a}_\text{1} &:& Ind\\
\text{c}_\text{dog} &:& \text{dog}(a_1)\\
\text{a}_\text{2} &:& Ind\\
\text{c}_\text{car} &:& \text{car}(a_2)\\
\text{c}_\text{left} &:& \text{left}(a_1, a_2)\\
\end{array}\right]\end{equation}


\section{Conclusions}

\subsection{Future work}

\bibliography{imagettr}
\end{document}