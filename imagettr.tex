\documentclass[11pt, a4paper]{article}

\usepackage{mlt-thesis-2015}

\usepackage[english]{babel}
\usepackage[hidelinks]{hyperref}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{comment}
\usepackage{caption}
\usepackage{listings}
\usepackage{textgreek}
\usepackage{lscape}
\lstset{language=Python}

\usepackage[acronym]{glossaries}
\glsdisablehyper

\title{Implementing perceptual and spatial semantics in type theory with records (TTR)}
%\subtitle{An implementation of visual perception and spatial relations in \gls{ttr}}
\author{Arild Matsson}
\date{}

\newacronym{hmm}{HMM}{Hidden Markov Model}
\newacronym{lstm}{LSTM}{Long Short-Term Memory}
\newacronym{cnn}{CNN}{convolutional neural network}
\newacronym{ttr}{TTR}{type theory with records}
\newacronym{nlg}{NLG}{natural language generation}
\newacronym{vqa}{VQA}{visual question answering}
\newacronym{yolo}{YOLO}{You only look once}
\newacronym{nlp}{NLP}{natural language processing}

\begin{document}

%% ============================================================================
%% Title page
%% ============================================================================
\begin{titlepage}

\maketitle

\vfill

\begingroup
\renewcommand*{\arraystretch}{1.2}
\begin{tabular}{l@{\hskip 20mm}l}
\hline
Master's Thesis & 30 credits\\
Programme & Master’s Programme in Language Technology\\
Level & Advanced level \\
Semester and year & Spring, 2018\\
Supervisors & Simon Dobnik and Staffan Larsson\\
Examiner & (name of the examiner)\\
Keywords & type theory, image recognition, object recognition, perceptual semantics, visual question answering
\end{tabular}
\endgroup

\thispagestyle{empty}
\end{titlepage}

%% ============================================================================
%% Abstract
%% ============================================================================
\newpage
\singlespacing
\glsresetall
\section*{Abstract}

\Gls{ttr} combines several theories of semantic modeling in a single framework.
[more ttr appraisal: type checking, robustness, verifiable...]
The present work suggests a \gls{ttr} model of perception, spatial cognition and language.
Utilizing PyTTR, a Python implementation of \gls{ttr}, the model is then implemented as an executable script.
%The Python implementation includes reformatting the output of an external image recognition system and parsing natural language.
Over pure Python programming, \gls{ttr} provides a transparent formal specification, as well as advanced typing and type checking.
The implementation is evaluated in a \acrlong{vqa} task.
The results confirm the suitability of this approach.
... as well as identifying some desired additions to PyTTR.
... machine learning??

% TTR also provides a connection between perception and a wide range of semantic phenomena described in TTR, e.g. quantification, inference, modality, negation, semantic coordination,.

\thispagestyle{empty}

%% ============================================================================
%% Preface
%% ============================================================================
%\newpage
%\section*{Preface}

%Acknowledgements, etc.

%\thispagestyle{empty}

%% ============================================================================
%% Contents
%% ============================================================================
\newpage

\begin{spacing}{0.0}
\glsresetall
\tableofcontents
\end{spacing}

\thispagestyle{empty}

%% ============================================================================
%% Introduction
%% ============================================================================
\newpage
\setcounter{page}{1}

\glsresetall
\section{Introduction}
\label{sec:intro}

Having computers ``understand'' images is desirable in several areas.
In this context, understanding means to process visual input on a cognitive level.
Sub-cognitive image processing may include tasks such as prominent color extraction, edge detection and visual pattern recognition.
Cognitive processing, however, includes identifying objects, their properties and their relations to each other, as well as including this information in further cognitive processes, such as reasoning and prediction [sources?].
To make meaning out of sensory (such as visual) input is known as \textit{perceptual semantics}.

[``Image understanding''] is interesting in various applications.
It is required in domestic assistant, driver-less cars and other camera-equipped (often mobile) robots, in order to obtain knowledge (or beliefs) and select appropriate actions.
The same kind of processing can also be used outside situated agents, for automatic processing of large sets of visual data [?].

[language, vqa]

[the creation of ttr]

[research question]
The goal of this thesis is to implement visual perception, cognition of spatial relations and basic natural language understanding in \gls{ttr}.
The implementation shall build on past relevant \gls{ttr} model proposals and focus on high-level completeness, meaning connecting vision and language through a cognitive model using \gls{ttr} as far as possible.
These are the points on which the result is finally qualitatively evaluated.
Lower level parts shall use existing solutions or basic placeholder implementations.

The theoretical background for this thesis is summarized in \textit{Background}.
In the \textit{Method} section, a model is defined and its implementation in Python and PyTTR described.
Then follows an account of the \textit{Results} with discussion, and finally some \textit{Conclusions}.



\glsresetall
\section{Background}
\label{sec:background}

This section will highlight some important pieces of the history of past research in relevant fields.

\subsection{Computational semantics}

[semantics]
Philosophers have long been interested in the study of meaning.
Frege.
[Frege?, formal semantics, Montague ...]

["classical" computational semantics]
Computational accounts of meaning emerged [how].
They are/were characterized by [what].
[Montague, ...]
[FOL]
\citep{BlackburnComputationalsemantics2003}


[modern methodologies]
With recent advancements in computer science, ambitious computational-semantic theories are now in abundance.
As a competitor to formal systems, statistical methods have emerged which do well in various tasks within semantics.
They utilize the performance of modern computers and leverage the large amounts of data that are available as a product of our largely digitalized society.


[formal systems]
If statistical models of semantics do well in [empirical/data-driven/practical] tasks, formal theories of semantics [do what?]
\cite{BlackburnComputationalsemantics2003}:
FOPC for the win, but ``other approaches are both possible and interesting''.





\subsubsection{Perceptual semantics}
% ... in general, and spatial relations in particular

\cite{Garnhamunifiedtheorymeaning1989}:
Left/right/above/below/front/behind.
Basic meaning (of spatial relational terms).
Framework vertical constraint.
Gravitation. NESW.
Clark: canonical encounter? (left of cabinet, left of chair.)
(Ships and outer space.)

\cite{LoganComputationalAnalysisApprehension1996} account for the state of 
Basic–deictic(–intrinsic) relations.
Perceptual–conceptual representation.
"Computational theory of apprehension": spatial indexing -> reference frame -> ... Instead, we move into the conceptual level, both in perception and language, and evaluate validity from there.
(Drawbacks?)
They present "evidence" for their theory, does that evidence contradict the present approach?

lo, refo? (refo used bleow)

Terms of spatial relations (\textit{behind}, \textit{to the left of}, etc.) have three types of meanings \citep{Garnhamunifiedtheorymeaning1989}: basic, deictic and intrinsical.
The deictic and intrinsical meanings hold for relations between two objects.
The deictic meaning is relative to the coordinate frame of the speaker, while the intrinsical is relative to that of the reference object.
The basic meaning, introduced by \cite{Garnhamunifiedtheorymeaning1989}, is also relative to the speaker, but holds for a single object only.
[and? (it is basic in the way that... what?) example?]

\cite{RegierGroundingspatiallanguage2001a}:
Four models. AVS.
Many experiments.
Spatial term ratings influenced by: proximal \& center-of-mass orientations, grazing line, distance.

\cite{CoventryClassificationExtrageometricInfluences2004}:
Extra-geometric constraints on the meaning of spatial relational terms.
Especially functional.
Functional geometric framework.
[functional aspect, Coventry?]

\cite{HarnadSymbolGroundingProblem1990}:
Can a computer really ``understand'' concepts, that is, will it operate on grounded symbols or just the (arbitrary) symbols themselves?
Turing test.

\cite{SteelsSymbolGroundingProblem2007}:
Peirce: object, symbol, concept. Grounded with method.
Searle and other claim that the Symbol grounding problem is insolvable.
Steels concludes that it is solved.
Using experiments where a number of agents participate in a language game where they make up random words for preset concepts and manage to ``agree'' on which words to use for which concepts.

\cite{AgrawalVQAVisualQuestion2015} suggest \acrfull{vqa} as a [complete] challenge in AI-completeness.
``A VQA system takes as input an image and a free-form, open-ended, natural-language question about the image and produces a natural-language answer as the output''.
The initiative includes a dataset and a series of annual competitions since 2016.
...



\subsection{Type theory in natural language processing}

Type theory was developed as an alternative to set theory, responding to paradoxes found in the latter.
With set theory widely serving as a \textit{foundation of mathematics}, the development of this new theory was [important].
Several different type theories have been created, of which Church's \textit{simply typed \textlambda-calculus} \cite{church40} and Martin-Löf's \textit{intuitionistic type theory} \citep{martinlof84} are some prominent examples.
\citep{CoquandTypeTheory2015} [too detailed?]

[use of type theory in nlp]

\cite{CooperRecordsRecordTypes2005} combines several theories from logic, semantics and linguistics in a single framework called \textbf{\acrfull{ttr}}.
 lambda calculus, phrase structure grammar, (DRT), (situation semantics) (what are the latter)?
It has been employed to model natural language in the context of dialogue, situated agents and spoken language.

[brief summary of TTR syntax?]



\subsubsection{Perceptual semantics in \gls{ttr}}

\cite{LarssonDialoguesHaveContent2011}

\cite{DobnikModellinglanguageaction2012}

\cite{lspc} model the point space perception of a mobile robot equipped with a rangefinder.
 that would move around and use laser range scanner or similar to collect points in space, group them into objects and detect spatial relations between them.
\Gls{ttr} is mainly used to model the transition from the perceptual to the cognitive domain as well as 
 throughout the model, accounting for perception and cognition.

\cite{ttrspat} develop this further, focusing especially on spatial relations.

\cite{LarssonFormalsemanticsperceptual2015}

Parsing: \cite{CooperRecordsRecordTypes2005}, \cite{RobinCooperAustiniantruthattitudes2005}, \cite{CooperTypetheorysemantics2012}, \cite{CooperTypetheorylanguage2016}



\subsubsection{PyTTR}

\cite{pyttr} is a Python implementation of \gls{ttr}.
It supports modeling records, record types, ptypes, functions and other TTR objects.
Additionally, it allows operations such as judgement, type checking and subtype checking.
As a Python library it also enables other features and peripheral procedures to be written in Python.

[the advantage of implementing a theoretical model]
PyTTR, itself being an implementation of TTR, allows, in turn, the implementation of models and theories that build on TTR.
By implementing a theoretical model as a computer program, it can ``come alive'' and be tested on real problems and data.
Another motivation for implementation is to compete with existing, less theoretically strong techniques to solve a given task, but it is possibly secondary to the former reason.

%[What has been written in PyTTR so far? nu, animat]



\subsection{Computer vision}

[computer vision]

[object recognition]

You only look once (YOLO) \citep{RedmonYouOnlyLook2015} is a neural network model that simultaneously predicts bounding boxes around objects and classifies the contained objects.

Among the most recent contributions to computer vision, Facebook's \textit{Detectron} \citep{Detectron2018} features outlining of identified objects and classification with impressive accuracy.

\begin{figure}[h]
\label{fig:dogbike_annotated}
\includegraphics[width=\textwidth]{dogbike_annotated}
\centering
\caption{Visualization of the labels and bounding boxes emitted by YOLO when given an image depicting a cyclist with a dog.}
\end{figure}

\subsubsection{\Acrfull{vqa}}



\section{Method}
\label{sec:method}

The work behind this thesis aims to produce a working implementation of a situated agent with visual as well as natural-language input, in \gls{ttr}.
Such an agent largely resembles those put forward in \cite{lspc} and \cite{ttrspat}, but there are some differences.
The present agent will feature:

\begin{enumerate}
\item Sensory input in the form of 2D images
\item Utilizing an external object recognition system
\item Apprehension of spatial relations
\item Basic natural language understanding
\end{enumerate}

While operational functionality and the overall procedural code is written in Python, the core model is written in \gls{ttr} (realized as Python code using PyTTR).
As such, \gls{ttr} serves as a formal specification language.
The additional layer provides formal transparency and type robustness.



\subsection{Theory development}
\label{ssec:theory}



\subsubsection{Objects}
% Distinguish different uses of "object": TTR a:T, Obj, IndObj. Perceptual vs conceptual.

% detected object vs. individuated object ?
% perceptual object vs. conceptual object ?

The perception of objects is largely based on \cite{lspc}.
There, the input space is a 3D point space rather than 2D images.
This difference necessitates other types for the perceptual input and the locations of perceived objects.



It may be important to note the overloading of the term ``object''.
In \gls{ttr}, an object is something that belongs to the extension of a type.
This is rather different from the semantic notion.
...

In computer vision, objects can refer to the results of object detection.
...

An object that has been detected in an image is, at the perceptual level, essentially modeled as a tuple of a location and a semantic property.

The $Obj$ type couples a property with a location, but it does not explicitly say anything about any individual object.
In \cite{lspc}, this corresponds to a \textit{perceptual} notion.
The step to the \textit{conceptual} domain is marked by generating a record type that corresponds to a situation, namely the situation that a certain individual has a certain property and is at a certain location.
[...]

There is a certain notable difference between the types $Obj$ and $IndObj$.
The former is a single record type, of which there will be many records.
Each record will hold information about a given detected object.
With the latter, it is the type itself that holds meaning, by describing a situation.
[...]

[propositions as types, true if there is an object of the type, situation semantics? \cite{BarwiseSituationsAttitudes1981}]
A record of a situation type then serves as a witness for the situation.
...

Create records on the fly.
Record types are fully specified.



\subsubsection{Spatial relations}
% Classification algorithm non-TTR. Simplistic, compare to sophisitcated alternatives.

Spatial relation classification is mostly based on \cite{ttrspat}, but more simplistic.

\cite{ttrspat} includes two important aspects that are not covered here.
First, it assumes a 3D point space as visual input, in contrast to the 2D image considered here.
Spatial relations in 3D crucially involves adapting the reference frame according to the viewpoint, while those are trivially fixed in the 2D case.
Second, it accounts for the functional aspect of spatial relationship, as detailed by \cite{CoventryClassificationExtrageometricInfluences2004}.




\subsubsection{Language}
% Why connect language? Is it VQA? Parsing. Classification before or after question.

[highlight how TTR makes it easy to answer a yes/no question]

A dog is to the left of a car
\begin{equation}\left[\begin{array}{rcl}
\text{x} &:& Ind\\
\text{y} &:& Ind\\
\text{c}_\text{dog} &:& \text{dog}(x)\\
\text{c}_\text{car} &:& \text{car}(y)\\
\text{c}_\text{left} &:& \text{left}(x, y)\\
\end{array}\right]\end{equation}


Essentially, we would like to check if the situation observed is a subtype of the situation described by the text/question, whether $Q \sqsupseteq A$. A new problem here is that field labels do not match, even if the field values (the types) match. We thus need to consider all (?) relabelings of $Q$:

A record type $T_1$ is a \textbf{relabel-subtype} of $T_2$, or $T_1 \sqsubseteq_{rlb} T_2$,  iff there is a relabeling of $T_1$, $T_{1_{rlb}}$ where $T_{1_{rlb}} \sqsubseteq T_2$.

(Or: iff $T_1$ is $\Sigma$-equivalent to a subtype of $T_2$?)

Could we forget field labels and just look at the two sets of field values? Not really, because we have dependent types, so $\text{dog}(x_1) ≠ \text{dog}(x_2)$. We need to carry out each candidate \textit{relabeling} and check subtypeness. In practice, and in this case, relabeling the basic-type ($Ind$) fields is enough, because those are the only ones whose labels appear in dependent fields. For each basic-field relabeling, we can then kind of forget labels and just find subtypeness of field values.

[classification before/after question]



\subsection{Implementation}
% How did I go about to implement this? PyTTR, YOLO, 

Much of the theory developed above is straightforwardly implemented in PyTTR.
Python is used mostly for binding the pieces together, but also some significant processes.



\subsubsection{External object recognizer}

YOLO \citep{yolo} is made with C but there is a Python wrapper.
When invoked from Python, the return value is a collection of dict objects with field for label, coordinates and a confidence score.
I made a Python function which takes this collection and translates it to a list of PyTTR $Obj$ records.



\subsubsection{Spatial classifiers}

Spatial classifiers $\kappa$ take two locations and return a boolean result.
I have implemented them as Python functions.
For the purpose of this thesis, no sophisticated spatial classification has been considered.
Instead, a naive comparison between centers of bounding boxes was implemented.
This was done for the four relations "left", "right", "above" and "below".



\subsubsection{Natural language parsing}

Parsing text to TTR has been addressed in \cite{CooperRecordsRecordTypes2005}, \cite{RobinCooperAustiniantruthattitudes2005}, \cite{CooperTypetheorysemantics2012}, \cite{CooperTypetheorylanguage2016}.
These accounts cover basic grammar, and are likely enough for the kind of utterances considered here.
It would have been interesting to see also those implemented here.
However, to save time I went for a simpler approach using NLTK's feature structure CFG.
With a custom Python function, the FOPC output is transformed to a TTR record type.



\subsubsection{Where PyTTR is not enough}

The individuation function, although expressible in TTR, is not possible to implement in PyTTR.
Therefore, it was implemented as a Python function named {\tt individualize}.

The \textit{relabel-subtype} relation, $\sqsubseteq_{rlb}$, was also implemented in Python.



\subsection{Evaluation}

[test on a few images and sentences] [or is one enough? it does what it does...]



\section{Results}
\label{sec:results}



\subsection{\Gls{ttr} model}

We begin the model formulation by declaring three basic types.

\begin{description}
\item [$Int$] An integer, such as 415.
\item [$Image$] A 2-dimensional digital image. It serves as an identifier to a set of extracted information, and its file type and actual data is not important in this thesis.
\item [$Ind$] A single individual object (or person), such as the reader or the Eiffel Tower.
\end{description}

A $Segment$ is a record type describing a rectangular bounding box within an (implicit) image (\autoref{eq:seg}).
$Ppty$ is the type of functions that can be applied to an individual and return a type (\autoref{eq:ppty}).
In our account the resulting type will be restricted to a ptype that is dependent on the individual, thus describing a property of it.

\begin{equation}\label{eq:seg}
Segment = \left[\begin{array}{rcl}
\text{cy} &:& Int\\
\text{cx} &:& Int\\
\text{w} &:& Int\\
\text{h} &:& Int
\end{array}\right]\end{equation}

\begin{equation}\label{eq:ppty}
Ppty = (Ind \rightarrow Type)\end{equation}

\subsubsection{Objects}

A detected object is modeled as a record of the type $Obj$ (\autoref{eq:obj}).
An example $Obj$ record is given in \autoref{eq:objrec}.
$Obj$ records are the result of performing \textit{object detection}.
This fact is expressed in TTR as the function type $ObjDetector$ (\autoref{eq:objdetector}).

\begin{equation}\label{eq:obj}
Obj = \left[\begin{array}{rcl}
\text{seg} &:& Segment\\
\text{pfun} &:& Ppty \\
\end{array}\right]\end{equation}

\begin{equation}\label{eq:objrec}
obj =
\left[\begin{array}{rcl}
\text{seg} &=& \left[\begin{array}{rcl}
\text{cx} &=& 138\\
\text{w} &=& 276\\
\text{cy} &=& 654\\
\text{h} &=& 809
\end{array}\right]\\
\text{pfun} &=& \lambda v:Ind\ .\ \text{person}(v)\\
\end{array}\right] : Obj\end{equation}

\begin{equation}\label{eq:objdetector}
ObjDetector = ( Image \rightarrow [Obj] )
\end{equation}

[...]

A situation type describing an individuated object is defined in \autoref{eq:indobj} as $IndObj$.
In this situation, $x$ is an individual and $loc$ is a location.
$cl$ specifies that $loc$ is the location of $x$, and the purpose of $cp$ is to declare a property of $x$.
%(Had TTR allowed it, we could have been more specific by stating that the field $cp$ should be of a ptype dependent on the field $x$.)

\begin{equation}\label{eq:indobj}
IndObj = \left[\begin{array}{rcl}
\text{x} &:& Ind \\
\text{loc} &:& Segment \\
\text{cp} &:& PType(\text{x}) [???] \\
\text{cl} &:& \text{location}(\text{x}, \text{loc}) \\
\end{array}\right]
\end{equation}

The function for generating an $IndObj$ subtype from an $Obj$ record is known from \cite{lspc} as an \textit{individuation function}.
It will be of the type $IndFun$.

\begin{equation}\label{eq:indfun}
IndFun = ( Obj \rightarrow RecType )
\end{equation}

The record type resulting from applying an $IndFun$ function should be a subtype of $IndObj$.

However, note that the $cp$ field is only being declared generally as $Type$.
Thus, it is \textit{subtypes} of $IndObj$, with a more specific typing of $cp$, that will be truly informative.
...

For each record type returned by the individuation function, a record is simultaneously created.
The $loc$ value of this record is naturally identical to the $seg$ value of the $Obj$ input record.
Objects for the remaining fields need to be instantiated on the spot.
Object creation is notated here as $A_{new}$, where the symbol $A$ may vary for the sake of readability.
%For the $x$ field, we create a new individual object $a_{new} : Ind$.
%For the ptype fields $cp$ and $cl$, we also create new objects $e_{new} : r.\text{pfun}(\text{x})$ and $e_{new} : \text{location}(\text{x}, \text{loc})$.

We are now ready to define the individuation function as in \autoref{eq:indfundef}, with an example application in \autoref{eq:indfunrec}.
The definition uses manifest fields to denote the \textit{fully specified} record type, or singleton record type.

\begin{equation}\label{eq:indfundef}
\lambda r : Obj\ . \left[\begin{array}{lcl}
    \text{x} = a_{new} &:& Ind \\
    \text{cp} = e_{new} &:& r.\text{pfun}(\text{x}) \\
    \text{cl} = e_{new} &:& \text{location}(\text{x}, \text{loc}) \\
    \text{loc} = r.\text{seg} &:& Segment\\
\end{array}\right]
\end{equation}

\begin{equation}\label{eq:indfunrec}
\left[\begin{array}{rcl}
\text{seg} &=& \left[\begin{array}{rcl}
\text{cx} &=& 138\\
\text{w} &=& 276\\
\text{cy} &=& 654\\
\text{h} &=& 809
\end{array}\right]\\
\text{pfun} &=& \lambda v:Ind\ .\ \text{person}(v)\\
\end{array}\right]
\mapsto
\left[\begin{array}{lcl}
    \text{x} = a_0 &:& Ind \\
    \text{cp} = e_0 &:& \text{person}(\text{x}) \\
    \text{cl} = e_1 &:& \text{location}(\text{x}, \text{loc}) \\
    \text{loc} = \left[\begin{array}{rcl}
\text{cx} &=& 138\\
\text{w} &=& 276\\
\text{cy} &=& 654\\
\text{h} &=& 809
\end{array}\right] &:& Segment\\
\end{array}\right]
\end{equation}



\subsubsection{Spatial relations}

Relations may hold between pairs of individuated objects.
How do we detect and model a certain relation between such a pair?

Since we are interested in the spatial relation between a \textit{reference object} and a \textit{located object}, we will be constructing tuple-like records of the type $LocTup$ defined in \autoref{eq:loctup}.
Records of this type contain instantiations of two $IndObj$ record types.
In \autoref{eq:clf}, a classifier is modeled as a function from such a record to a new record type which should describe the relation.

\begin{equation}\label{eq:loctup}
LocTup = \left[\begin{array}{rcl}
    \text{lo} &:& IndObj \\
    \text{refo} &:& IndObj \\
    \end{array}\right]
\end{equation}

\begin{equation}\label{eq:clf}
ClfFun = ( LocTup \rightarrow RecType )
\end{equation}

For instance, a classifier for ``to the left of'' might look like in \autoref{eq:leftclfdef}, where $\kappa_{left}$ is a non-TTR, boolean function.
Of course, the requirement that the individual $r.\text{lo}.\text{x}$ is actually located at $r.\text{lo}.\text{loc}$ (and same for $r.\text{refo}$) is implicit from the typing as $IndObj$, where the field $\text{cl} : \text{location}(\text{x}, \text{loc})$ is necessarily present.

\begin{equation}\label{eq:leftclfdef}
\lambda r : LocTup \ .\ 
\begin{cases}
\left[\begin{array}{rcl}
    \text{cr} &:& \text{left}(r.\text{lo}.\text{x}, r.\text{refo}.\text{x}) \\
\end{array}\right],
& \text{if } \kappa_{left}(r.\text{lo}.\text{loc}, r.\text{refo}.\text{loc}) \\
[], & \text{otherwise}
\end{cases}
\end{equation}



\subsubsection{Agent}

The perceptual-conceptual pieces described above are here combined.
We are building an agent who receives classified and located objects of an image, apprehends their basic status and spatial relations, and validates natural-language propositions.

\begin{equation}\label{eq:agent}
Agent = \left[\begin{array}{rcl}
    \text{objdetector} &:& ObjDetector \\
    \text{indfun} &:& IndFun \\
    \text{appr} &:& [(Rec \rightarrow RecType)] \\
    \text{state} &:& AgentState \\
    \end{array}\right]
\end{equation}

\begin{equation}\label{eq:state}
AgentState = \left[\begin{array}{rcl}
    \text{img} &:& Image \\
    \text{perc} &:& [Obj] \\
    \text{bel} &:& [RecType] \\
    \text{utt} &:& RecType \\
    \end{array}\right]
\end{equation}

The fields $objdetector$, $indfun$ and $appr$ of $Agent$ are to be statically defined for a specific agent.
While running, the agent will modify the $AgentState$ record in $state$.

\begin{enumerate}
\item Visual input in the form of an image is received and assigned to $agt.\text{state}.\text{img}$.
\item $objdetector$ is invoked on $agt.\text{state.img}$ and creates a collection of records that are assigned to $agt.\text{state}.\text{perc}$.
\item $indfun$ is, in turn, invoked on each record in $agt.\text{state.perc}$ and resulting record types are added to $agt.\text{state.bel}$.
\item Now, the functions in $agt.\text{appr}$ are applied to type-valid combinations of $agt.\text{state.bel}$ records, and resulting record types are added to $agt.\text{state.bel}$
	\begin{enumerate}
	\item Define ``type-valid combinations''? How detailed?
	\end{enumerate}
\item Any language input is parsed and the resulting record type assigned to $agt.\text{state.utt}$.
\item The record types in $agt.\text{state.bel}$ are combined/concatenated. If the resulting record type is a relabel-subtype of $agt.\text{state.utt}$, a positive answer is emitted; otherwise a negative answer is emitted.
\end{enumerate}

An example state of an agent $agt$ is shown in \autoref{eq:agt}.

\begin{landscape}
\begin{equation}\label{eq:agt}
\renewcommand{\arraystretch}{1.2}
agt = \left[\begin{array}{rcl}
    \text{objdetector} &=& \mathtt{yolo\_detector} \\
    \text{indfun} &=& \mathtt{individualize} \\
    \text{appr} &=& [Clf_{left}, Clf_{right}, Clf_{above}, Clf_{below}, ...] \\
    \text{state} &=& \left[\begin{array}{rcl}
		\text{img} &=& \mathtt{dogride.jpg} \\
		\text{perc} &=& [
			\left[\begin{array}{rcl}
				\text{seg} &=& \left[\begin{array}{rcl}
					\text{w} &=& 197\\
					\text{cx} &=& 452\\
					\text{h} &=& 351\\
					\text{cy} &=& 261
					\end{array}\right]\\
				\text{pfun} &=& \lambda a:Ind\ .\ \text{person}(a)
				\end{array}\right],
			\left[\begin{array}{rcl}
				\text{seg} &=& \left[\begin{array}{rcl}
					\text{w} &=& 422\\
					\text{cx} &=& 435\\
					\text{h} &=& 242\\
					\text{cy} &=& 355
					\end{array}\right]\\
				\text{pfun} &=& \lambda a:Ind\ .\ \text{bicycle}(a)
				\end{array}\right],
			...
			] \\
		\text{bel} &=& \begin{array}{l} [
			\left[\begin{array}{rcl}
				\text{x} = a_0 &:& Ind\\
				\text{cp} = e_0 &:& \text{person}(x)\\
				\text{cl} = e_{1} &:& \text{location}(x, loc)\\
				\text{loc} = \left[\begin{array}{rcl}
					\text{w} &=& 197\\
					\text{cx} &=& 452\\
					\text{h} &=& 351\\
					\text{cy} &=& 261
					\end{array}\right]
					&:& Segment \\
				\end{array}\right],
			{}{} \left[\begin{array}{rcl}
				\text{cr}=e_6 &:& \text{above}(a_{0}, a_{1})
				\end{array}\right],
			... ]
			\end{array} \\
		\text{utt} &=& \left[\begin{array}{rcl}
			\text{x} &:& Ind\\
			\text{y} &:& Ind\\
			\text{c}_\text{0} &:& \text{dog}(x)\\
			\text{c}_\text{1} &:& \text{bicycle}(y)\\
			\text{c}_\text{2} &:& \text{left}(x, y)\\
			\end{array}\right] \\
		\end{array}\right] \\
    \end{array}\right]
\end{equation}
\end{landscape}



\subsection{Discussion}
\label{sec:discussion}

Contradiction of Logan \& Sadler's "evidence" for their "theory of apprehension" (which is different from mine)? (Already Regier \& Carlson did.)

Not a full VQA solution.
It can only answer one question type, and only in the form "A P is R a Q", which is not even a question.
With only the extension of parsing, it could understand (= educe situation record types) more complex forms like "A R1 B1 and R2 B2", "A1 R1 B1 and A2 R2 B2".
"What is R B?"
"What color is the A?"
"How many A are there?" etc.

...

PyTTR extensions (here or Conclusions?)

Functional aspect (Coventry).



\section{Conclusions}
\label{sec:conclusions}

It works and it's nice because...
robustness, type checking, verifiable (?)



\subsection{Future work}

Spatial templates \& regions of acceptability. Compound relations (above right) finer (directly). Functional aspect.  \cite{LoganComputationalAnalysisApprehension1996} (also Dobnik etc)

Basic->Deictic->Intrinsic relations  \cite{LoganComputationalAnalysisApprehension1996}. 3D.

4 question types.
Moar question types (tasks/programs/routines in L\&S).

Dialogue.

Classification after Q.

Probabilistic TTR for ``good fit'' (is is more to the left or more above?).

Negation.



\bibliography{imagettr}
\end{document}