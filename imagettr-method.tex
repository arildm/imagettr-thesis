\section{Method}
\label{sec:method}

The work behind this thesis aims to produce a working implementation of an application with visual as well as natural-language input, in \gls{ttr}.
Such an application largely resembles those put forward in \cite{lspc} and \cite{ttrspat}, but there are some differences.
The present application will feature:

\begin{enumerate}
\item Sensory input in the form of 2D images
\item Utilizing an external object recognition system
\item Basic apprehension of spatial relations
\item Basic natural language understanding
\end{enumerate}

While operational functionality and the overall procedural code is written in Python, the core model is written in \gls{ttr} (realized as Python code using PyTTR).
As such, \gls{ttr} serves as a formal specification language.
The additional layer provides formal transparency and type robustness.



\subsection{Theory development}
\label{ssec:theory}



\subsubsection{Objects}
% Distinguish different uses of "object": TTR a:T, Obj, IndObj. Perceptual vs conceptual.

% detected object vs. individuated object ?
% perceptual object vs. conceptual object ?

The perception of objects is largely based on \cite{lspc}.
There, the input space is a 3D point space rather than 2D images.
This difference necessitates other types for the perceptual input and the locations of perceived objects.



It may be important to note the overloading of the term ``object''.
In \gls{ttr}, an object is something that belongs to the extension of a type.
This is rather different from the semantic notion.
...

In computer vision, objects can refer to the results of object detection.
...

An object that has been detected in an image is, at the perceptual level, essentially modeled as a tuple of a location and a semantic property.

The $Obj$ type couples a property with a location, but it does not explicitly say anything about any individual object.
In \cite{lspc}, this corresponds to a \textit{perceptual} notion.
The step to the \textit{conceptual} domain is marked by generating a record type that corresponds to a situation, namely the situation that a certain individual has a certain property and is at a certain location.
[...]

There is a certain notable difference between the types $Obj$ and $IndObj$.
The former is a single record type, of which there will be many records.
Each record will hold information about a given detected object.
With the latter, it is the type itself that holds meaning, by describing a situation.
[...]

Perceptual object records were defined by declaring its type, but that is useless here.
The type of all record types is $RecType$, which says nothing about its fields.

[propositions as types, true if there is an object of the type, situation semantics? \cite{BarwiseSituationsAttitudes1981}]
A record of a situation type then serves as a witness for the situation.
...

Create records on the fly.
Record types are fully specified.



\subsubsection{Spatial relations}
% Classification algorithm non-TTR. Simplistic, compare to sophisitcated alternatives.

Spatial relation classification is mostly based on \cite{ttrspat}, but more simplistic.

\cite{ttrspat} includes two important aspects that are not covered here.
First, it assumes a 3D point space as visual input, in contrast to the 2D image considered here.
Spatial relations in 3D crucially involves adapting the reference frame according to the viewpoint, while those are trivially fixed in the 2D case.
Second, it accounts for the functional aspect of spatial relationship, as detailed by \cite{CoventryClassificationExtrageometricInfluences2004}.




\subsubsection{Language}
% Why connect language? Is it VQA? Parsing. Classification before or after question.

[highlight how TTR makes it easy to answer a yes/no question]

A dog is to the left of a car
\begin{equation}\left[\begin{array}{rcl}
\text{x} &:& Ind\\
\text{y} &:& Ind\\
\text{c}_\text{dog} &:& \text{dog}(x)\\
\text{c}_\text{car} &:& \text{car}(y)\\
\text{c}_\text{left} &:& \text{left}(x, y)\\
\end{array}\right]\end{equation}


Essentially, we would like to check if the situation observed is a subtype of the situation described by the text/question, whether $Q \sqsupseteq A$. A new problem here is that field labels do not match, even if the field values (the types) match. We thus need to consider all (?) relabelings of $Q$:

A record type $T_1$ is a \textbf{relabel-subtype} of $T_2$, or $T_1 \sqsubseteq_{rlb} T_2$,  iff there is a relabeling of $T_1$, $T_{1_{rlb}}$ where $T_{1_{rlb}} \sqsubseteq T_2$.

(Or: iff $T_1$ is $\Sigma$-equivalent to a subtype of $T_2$?)

Could we forget field labels and just look at the two sets of field values? Not really, because we have dependent types, so $\text{dog}(x_1) â‰  \text{dog}(x_2)$. We need to carry out each candidate \textit{relabeling} and check subtypeness. In practice, and in this case, relabeling the basic-type ($Ind$) fields is enough, because those are the only ones whose labels appear in dependent fields. For each basic-field relabeling, we can then kind of forget labels and just find subtypeness of field values.

[classification before/after question]



\subsection{Implementation}
% How did I go about to implement this? PyTTR, YOLO, 

Much of the theory developed above is straightforwardly implemented in PyTTR.
Python is used mostly for binding the pieces together, but also some significant processes.



\subsubsection{External object recognizer}

YOLO \citep{yolo} is made with C but there is a Python wrapper.
When invoked from Python, the return value is a collection of dict objects with field for label, coordinates and a confidence score.
I made a Python function which takes this collection and translates it to a list of PyTTR $Obj$ records.



\subsubsection{Spatial classifiers}

Spatial classifiers $\kappa$ take two locations and return a boolean result.
I have implemented them as Python functions.
For the purpose of this thesis, no sophisticated spatial classification has been considered.
Instead, a naive comparison between centers of bounding boxes was implemented.
This was done for the four relations "left", "right", "above" and "below".



\subsubsection{Natural language parsing}

Parsing text to TTR has been addressed in \cite{CooperRecordsRecordTypes2005}, \cite{RobinCooperAustiniantruthattitudes2005}, \cite{CooperTypetheorysemantics2012}, \cite{CooperTypetheorylanguage2016}.
These accounts cover basic grammar, and are likely enough for the kind of utterances considered here.
It would have been interesting to see also those implemented here.
However, to save time I went for a simpler approach using NLTK's feature structure CFG.
With a custom Python function, the FOPC output is transformed to a TTR record type.



\subsubsection{Where PyTTR is not enough}

The individuation function, although expressible in TTR, is not possible to implement in PyTTR.
Therefore, it was implemented as a Python function named {\tt individualize}.

The \textit{relabel-subtype} relation, $\sqsubseteq_{rlb}$, was also implemented in Python.



\subsection{Evaluation}

The system is tested on a few sentences for a few images.
...
