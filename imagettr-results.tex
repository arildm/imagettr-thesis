\section{Results}
\label{sec:results}

In this section, the model is defined.
First formally, in \gls{ttr}.
Then in Python, using PyTTR but also with some passages of non-PyTTR Python code.
The implementation necessitated some extensions to PyTTR which are also presented.
[The vqa application is presented? sd] [the problem is that currently there is no vqa application...]



\subsection{\Gls{ttr} model}
\label{ssec:ttrmodel}

Three basic types exist in the model.

\begin{description}
\item [$Ind$] A single individual object (or person), such as the reader or the Eiffel Tower.
\item [$Int$] An integer, such as 415.
\item [$Image$] A 2-dimensional digital image. It serves as an identifier to a set of extracted information, and its file type and actual data is not important in this thesis.
\end{description}

A $Segment$ is a record type describing a rectangular bounding box within an (implicit) image (\autoref{eq:seg}).
Its fields contain the center coordinates of the box ($cx$ and $cy$) and the width ($w$) and height ($h$) of the box.
$Ppty$ is the type of functions that can be applied to an individual and return a type (\autoref{eq:ppty}).
In our account the resulting type will be restricted to a ptype that is dependent on the individual, thus describing a property of it.

\begin{equation}\label{eq:seg}
Segment = \left[\begin{array}{rcl}
\text{cx} &:& Int\\
\text{cy} &:& Int\\
\text{w} &:& Int\\
\text{h} &:& Int
\end{array}\right]\end{equation}

\begin{equation}\label{eq:ppty}
Ppty = (Ind \rightarrow Type)\end{equation}

[PTy?]

A perceptual object is a record of the type $Obj$ (\autoref{eq:obj}).
An example record is given in \autoref{eq:objrec}.
%$Obj$ records are the result of performing \textit{object detection}.
%This fact is expressed in TTR as the function type $ObjDetector$ (\autoref{eq:objdetector}).
An object detector is a function from an image to a set of perceptual objects, as captured by the $ObjDetector$ function type (\autoref{eq:objdetector}).

[differences/similarites LSPC and others]

\begin{equation}\label{eq:obj}
Obj = \left[\begin{array}{rcl}
\text{seg} &:& Segment\\
\text{pfun} &:& Ppty \\
\end{array}\right]\end{equation}

\begin{equation}\label{eq:objrec}
obj =
\left[\begin{array}{rcl}
\text{seg} &=& \left[\begin{array}{rcl}
\text{cx} &=& 138\\
\text{w} &=& 276\\
\text{cy} &=& 654\\
\text{h} &=& 809
\end{array}\right]\\
\text{pfun} &=& \lambda v:Ind\ .\ \text{person}(v)\\
\end{array}\right] : Obj\end{equation}

\begin{equation}\label{eq:objdetector}
ObjDetector = ( Image \rightarrow [Obj] )
\end{equation}



\subsubsection{Individuation}

The perceptual object couples a property with a location, but it does not explicitly say anything about any individual object.
In \cite{lspc}, the step from the perceptual to the \textit{conceptual} domain is made by generating a record type that corresponds to a situation, namely the situation that a certain individual has a certain property and is at a certain location.
This situation record type is known as an \textit{individuated object}, and is a subtype of $IndObj$ (\autoref{eq:indobj}).
Here, $x$ is an individual and $loc$ is a location.
$cl$ specifies that $loc$ is the location of $x$, and the purpose of $cp$ is to declare a property of $x$.
$PTy$ is defined as a supertype of all ptypes (\autoref{eq:pty}).

\begin{equation}\label{eq:indobj}
IndObj = \left[\begin{array}{rcl}
\text{x} &:& Ind \\
\text{loc} &:& Segment \\
\text{cp} &:& PTy \\
\text{cl} &:& \text{location}(\text{x}, \text{loc}) \\
\end{array}\right]
\end{equation}

\begin{equation}\label{eq:pty}
PTy : Type
\end{equation}

A function for generating an $IndObj$ subtype from an $Obj$ record is known from \cite{lspc} as an \textit{individuation function}.
It is typed as $IndFun$ (\autoref{eq:indfun}).

\begin{equation}\label{eq:indfun}
IndFun = ( Obj \rightarrow RecType )
\end{equation}

The record type resulting from applying an $IndFun$ function should be a subtype of $IndObj$.

For each record type returned by the individuation function, a record is simultaneously created.
The $loc$ value of this record is naturally identical to the $seg$ value of the $Obj$ input record.
Objects for the remaining fields need to be instantiated on the spot.
Object creation is notated here as $A_{new}$, where the symbol $A$ may vary for the sake of readability.
%For the $x$ field, we create a new individual object $a_{new} : Ind$.
%For the ptype fields $cp$ and $cl$, we also create new objects $e_{new} : r.\text{pfun}(\text{x})$ and $e_{new} : \text{location}(\text{x}, \text{loc})$.

The individuation function is defined in \autoref{eq:indfundef}, with an example application in \autoref{eq:indfunrec}.
The definition uses manifest fields to denote the \textit{fully specified} record type, or singleton record type.

\begin{equation}\label{eq:indfundef}
f_{IndFun} = \lambda r : Obj\ . \left[\begin{array}{lcl}
    \text{x} = a_{new} &:& Ind \\
    \text{cp} = e_{new_1} &:& r.\text{pfun}(\text{x}) \\
    \text{cl} = e_{new_2} &:& \text{location}(\text{x}, \text{loc}) \\
    \text{loc} = r.\text{seg} &:& Segment\\
\end{array}\right]
\end{equation}

\begin{equation}\label{eq:indfunrec}
f_{IndFun}(
\left[\begin{array}{rcl}
\text{seg} &=& \left[\begin{array}{rcl}
\text{cx} &=& 138\\
\text{w} &=& 276\\
\text{cy} &=& 654\\
\text{h} &=& 809
\end{array}\right]\\
\text{pfun} &=& \lambda v:Ind\ .\ \text{person}(v)\\
\end{array}\right]
) =
\left[\begin{array}{lcl}
    \text{x} = a_0 &:& Ind \\
    \text{cp} = e_0 &:& \text{person}(\text{x}) \\
    \text{cl} = e_1 &:& \text{location}(\text{x}, \text{loc}) \\
    \text{loc} = \left[\begin{array}{rcl}
		\text{cx} &=& 138\\
		\text{w} &=& 276\\
		\text{cy} &=& 654\\
		\text{h} &=& 809
		\end{array}\right] &:& Segment\\
\end{array}\right]
\end{equation}



\subsubsection{Spatial relations}

Relations may hold between pairs of individuated objects.
How do we detect and model a certain relation between such a pair?

Since we are interested in the spatial relation between a \textit{reference object} and a \textit{located object}, we will be constructing tuple-like records of the type $LocTup$ defined in \autoref{eq:loctup}.
Records of this type contain instantiations (records) of two $IndObj$ record types.
In \autoref{eq:clf}, a classifier is modeled as a function from such a record to a new record type which should describe the relation.

\begin{equation}\label{eq:loctup}
LocTup = \left[\begin{array}{rcl}
    \text{lo} &:& IndObj \\
    \text{refo} &:& IndObj \\
    \end{array}\right]
\end{equation}

\begin{equation}\label{eq:clf}
ClfFun = ( LocTup \rightarrow RecType )
\end{equation}

For instance, a classifier for ``left'' might look like in \autoref{eq:leftclfdef}, where $\kappa_{left}$ is a non-TTR, boolean function.
Of course, the requirement that the individual $r.\text{lo}.\text{x}$ is actually located at $r.\text{lo}.\text{loc}$ (and same for $r.\text{refo}$) is implicit from the typing as $IndObj$, where the field $\text{cl} : \text{location}(\text{x}, \text{loc})$ is necessarily present.

\begin{equation}\label{eq:leftclfdef}
\lambda r : LocTup \ .\ 
\begin{cases}
\left[\begin{array}{rcl}
    \text{cr} &:& \text{left}(r.\text{lo}.\text{x}, r.\text{refo}.\text{x}) \\
\end{array}\right],
& \text{if } \kappa_{left}(r.\text{lo}.\text{loc}, r.\text{refo}.\text{loc}) \\
[], & \text{otherwise}
\end{cases}
\end{equation}



\subsubsection{Combining beliefs}

The hitherto generated types are considered our beliefs.
As described in \autoref{ssec:languagevqa}, we can answer to a polar question if we \textit{combine} the beliefs to a scene type and check whether it is a subtype of the question type, $S \sqsubseteq Q$.
How is this combining carried out?
A direct application of the \textit{merge} operation is not suitable for this, as we have labels reocurring in multiple record types.
Instead, all record types are relabeled with new, unique labels, before being merged.

\label{def:cfmerge}
If $T_1$ and $T_2$ are record types, then the \textbf{conflict-free merge} $T_1 \underset{u}{\wedge} T_2$ is a merge of the record types relabeled such that they do not share any labels.

If the belief types are $T_1, T_2, ..., T_n$, they are combined to $T_1 \underset{u}{\wedge} T_2 \underset{u}{\wedge} ... \underset{u}{\wedge} T_n$.



\subsubsection{Agent}

We are now connecting the perceptual-conceptuel pieces described above, by building an agent.
It receives information on classified and located objects of an image, and apprehends their basic status and spatial relations.
It also receives the result of parsing a natural-language utterance.
The information is in \gls{ttr} form, which finally allows the agent to connect the two modes.
This provides a means to answer to natural-language questions about the image.

\begin{equation}\label{eq:agent}
Agent = \left[\begin{array}{rcl}
    \text{objdetector} &:& ObjDetector \\
    \text{indfun} &:& IndFun \\
    \text{appr} &:& [(Rec \rightarrow RecType)] \\
    \text{state} &:& AgentState \\
    \end{array}\right]
\end{equation}

\begin{equation}\label{eq:state}
AgentState = \left[\begin{array}{rcl}
    \text{img} &:& Image \\
    \text{perc} &:& [Obj] \\
    \text{bel} &:& [RecType] \\
    \text{utt} &:& RecType \\
    \end{array}\right]
\end{equation}

The fields $objdetector$, $indfun$ and $appr$ of $Agent$ are to be statically defined for a specific agent.
While running, the agent will modify the $AgentState$ record in $state$.

For an agent $agt : Agent$, the perception and question-answering procedure is carried out as follows.

\begin{enumerate}
\item Visual input in the form of an image is received and assigned to $agt.\text{state}.\text{img}$.
\item $objdetector$ is invoked on $agt.\text{state.img}$ and creates a collection of records that are assigned to $agt.\text{state}.\text{perc}$.
\item $indfun$ is, in turn, invoked on each record in $agt.\text{state.perc}$ and resulting record types are added to $agt.\text{state.bel}$.
\item Now, each function in $agt.\text{appr}$ are applied:
	\begin{enumerate}
	\item The fields of the domain record type of the function is considered its arguments.
	\item Each combination of $agt.\text{state.bel}$ record types that matches the argument types is considered for input.
	\item A record is instantiated for each input record type, and the records are combined into one that matches the domain record type of the function.
	\item Resulting record types are added to $agt.\text{state.bel}$
	\end{enumerate}
	For example, the \textit{left} classifier in \autoref{eq:leftclfdef} is applied to each pair of $IndObj$ after instantiating and combining records into a $LocTup$.
\item Any language input is parsed and the resulting record type assigned to $agt.\text{state.utt}$.
\item The record types in $agt.\text{state.bel}$ are combined. If the resulting record type is a relabel-subtype of $agt.\text{state.utt}$, the answer ``yes'' is emitted; otherwise ``no''.
\end{enumerate}

An example state of an agent $agt$ is shown in \autoref{eq:agt}.

\begin{landscape}
\begin{equation}\label{eq:agt}
\renewcommand{\arraystretch}{1.2}
agt = \left[\begin{array}{rcl}
    \text{objdetector} &=& \mathtt{yolo\_detector} \\
    \text{indfun} &=& \mathtt{indfund} \\
    \text{appr} &=& [Clf_{left}, Clf_{right}, Clf_{above}, Clf_{below}] \\
    \text{state} &=& \left[\begin{array}{rcl}
		\text{img} &=& \mathtt{dogride.jpg} \\
		\text{perc} &=& [
			\left[\begin{array}{rcl}
				\text{seg} &=& \left[\begin{array}{rcl}
					\text{w} &=& 197\\
					\text{cx} &=& 452\\
					\text{h} &=& 351\\
					\text{cy} &=& 261
					\end{array}\right]\\
				\text{pfun} &=& \lambda a:Ind\ .\ \text{person}(a)
				\end{array}\right],
			\left[\begin{array}{rcl}
				\text{seg} &=& \left[\begin{array}{rcl}
					\text{w} &=& 422\\
					\text{cx} &=& 435\\
					\text{h} &=& 242\\
					\text{cy} &=& 355
					\end{array}\right]\\
				\text{pfun} &=& \lambda a:Ind\ .\ \text{bicycle}(a)
				\end{array}\right],
			...
			] \\
		\text{bel} &=& \begin{array}{l} [
			\left[\begin{array}{rcl}
				\text{x} = a_0 &:& Ind\\
				\text{cp} = e_0 &:& \text{person}(x)\\
				\text{cl} = e_{1} &:& \text{location}(x, loc)\\
				\text{loc} = \left[\begin{array}{rcl}
					\text{w} &=& 197\\
					\text{cx} &=& 452\\
					\text{h} &=& 351\\
					\text{cy} &=& 261
					\end{array}\right]
					&:& Segment \\
				\end{array}\right],
			{}{} \left[\begin{array}{rcl}
				\text{cr}=e_6 &:& \text{above}(a_{0}, a_{1})
				\end{array}\right],
			... ]
			\end{array} \\
		\text{utt} &=& \left[\begin{array}{rcl}
			\text{x} &:& Ind\\
			\text{y} &:& Ind\\
			\text{c}_\text{0} &:& \text{dog}(x)\\
			\text{c}_\text{1} &:& \text{bicycle}(y)\\
			\text{c}_\text{2} &:& \text{left}(x, y)\\
			\end{array}\right] \\
		\end{array}\right] \\
    \end{array}\right]
\end{equation}
\end{landscape}



\subsection{Python implementation}
\label{ssec:python}

This section presents significant parts of the Python implementation of the model described above.
The full code, including visualization and more comments, is published as a Jupyter Notebook at \url{https://github.com/arildm/imagettr}.



\subsubsection{PyTTR definitions}

\begin{lstlisting}[label={lst:pyttrbasic}, caption=TTR type definitions]
Ind = BType('Ind')

Int = BType('Int')
Int.learn_witness_condition(lambda x: isinstance(x, int))

Image = BType('Image')
Image.learn_witness_condition(lambda x: isinstance(x, PIL.Image.Image))

Segment = RecType({'cx': Int, 'cy': Int, 'w': Int, 'h': Int})
Ppty = FunType(Ind, Ty)
Obj = RecType({'seg': Segment, 'pfun': Ppty})
Objs = ListType(Obj)
ObjDetector = FunType(Image, Objs)
ObjDetector.witness_cache.append(yolo_detector)

PTy = Type('PTy')
PTy.learn_witness_condition(lambda p: isinstance(p, HypObj) \
    and forsome(p.types, lambda t: isinstance(t, PType)))

IndObj = RecType({
    'x' : Ind,
    'loc' : Segment,
    'cp' : PTy,
    'cl' : create_fun('location', 'ab').app('x').app('loc'),
})
IndFun = FunType(Obj, RecTy)

LocTup = RecType({'lo': IndObj, 'refo': IndObj})
ClfRes = RecType({'cr': PTy})
RelClf = FunType(LocTup, ClfRes)
\end{lstlisting}



\subsubsection{Application procedure}

No implementation has been done of the agent described above.
Instead, the procedure for object detection, spatial classification and question answering is performed by the lines of code in \autoref{lst:procedure}.
The functions used here are partially described in the following subsections.

\begin{lstlisting}[label=lst:procedure, caption=Application procedure]
objs = list(yolo_detector(img))
indobjs = [indfun(r) for r in objs]
rels = list(find_all_rels(indobjs))
bel = indobjs + rels
q = eng_to_pyttr(text)
ans = bool(find_subtype_relabeling(combine_beliefs(bel), q))
\end{lstlisting}



\subsubsection{Outside PyTTR}

\paragraph{Individuation function}

The PyTTR implementation of \gls{ttr} functions does not support usage of the argument within the function body.
The individuation function was therefore implemented in Python.
To keep the connection to \gls{ttr} tight, the argument and the result are checked against their \gls{ttr} types.

\begin{lstlisting}[label={lst:indfun},caption={Individuation function}]
def indfun(r):
    if not Obj.query(r):
        raise ValueError()
    cp = r.pfun.app('x')
    cl = create_fun('location', 'ab').app('x').app('loc')
    indobj = RecType({
        'x': SingletonType(Ind, Ind.create()),
        'cp': SingletonType(cp, cp.create()),
        'loc': SingletonType(Segment, r.seg),
        'cl': SingletonType(cl, cl.create()),
    })
    if not unsingleton(indobj).subtype_of(IndObj):
        raise ValueError()
    return indobj
IndFun.witness_cache.append(indfun)

indobjs = [indfun(r) for r in objs]
\end{lstlisting}

\paragraph{Spatial relation classification}

In \autoref{lst:relclf}, each pair of $IndObj$ types is tested for each spatial relation classifier.
A classifier is a tuple of a predicate identifier (e.g. {\tt left}) and a function from two $Segment$s to a boolean value.
The return values of {\tt get\_relclfs} are functions of the same kind as the example in \autoref{eq:leftclfdef}.

\begin{lstlisting}[label=lst:relclf, caption=Spatial relation classifiers]
def get_relclfs():
    for pred, f in location_relation_classifiers.items():
        def relclf(r):
            if f(r.lo.loc, r.refo.loc):
                c = create_fun(pred, 'ab').app(r.lo.x).app(r.refo.x)
                return RecType({'cr': SingletonType(c, c.create())})
            return RecType()
        RelClf.witness_cache.append(relclf)
        yield relclf

def find_all_rels(indobjs):
    """Find all relations between IndObj records."""
    for relclf in get_relclfs():
        for loT, refoT in product(indobjs, indobjs):
            loctup = Rec({'lo': loT.create(), 'refo': refoT.create()})
            yield relclf(loctup)
\end{lstlisting}

\paragraph{YOLO}

In \autoref{lst:yolo}, YOLO object detection is invoked on an image, and the result is converted to PyTTR records.

\begin{lstlisting}[label=lst:yolo, caption=YOLO usage]
def yolo_detector(i):
    """Creates IndObj records for YOLO results."""
    for o in yolo(i):
        o = yolo_reformat(o)
        yield Rec({
            'seg': Rec(o['loc']),
            'pfun': create_fun(o['label'].replace(' ', '_')),
        })
\end{lstlisting}



\subsubsection{Extensions to PyTTR}

The combining of beliefs and the subtype relabeling require TTR operations that are not defined in PyTTR.

\paragraph{Combining beliefs}

The conflict-free merge operation defined in \autoref{def:cfmerge} is implemented in \autoref{lst:mergeunconflict}.
For example, if $T_1 = T_2 = [x:Ind]$, then $\mathtt{merge\_unconflict}(T_1, T_2)$ evaluates to $\left[\begin{array}{rcl} x_0&:&Ind \\ x_1&:&Ind \end{array}\right]$ (or a similar record type with different subscript indices).

\begin{lstlisting}[label={lst:mergeunconflict},caption={merge\_unconflict}]
def unique_labels(T):
    """Relabel a RecType so each field label is unique over all RecTypes."""
    for l, v in T.comps.__dict__.items():
        if '_' not in l:
            T.Relabel(l, gensym(l))
    return T

def merge_unconflict(T1, T2):
    """Merge two RecTypes after making sure they do not share any field labels."""
    T1c = unique_labels(copy_rectype(T1))
    T2c = unique_labels(copy_rectype(T2))
    return T1c.merge(T2c)
\end{lstlisting}

When it comes to dependent types, a custom in \gls{ttr} literature is to include the dependum as a field and use the field label in the dependent type.
In our implementation, spatial classifiers return record types where dependums are instead specified as individuals directly.
The function in \autoref{lst:useindfieldlabels} helps when those record types are combined with others.

\begin{lstlisting}[label={lst:useindfieldlabels},caption={use\_ind\_field\_labels}]
def use_ind_field_labels(T):
    """c:foo(a) becomes c:foo(x) if x=a:Ind is present."""
    T = copy_rectype(T)
    for l, v in T.comps.__dict__.items():
        if isinstance(v, SingletonType) and v.comps.base_type == Ind:
            a = v.comps.obj
            T.Relabel(a, l)
            # Undo the relabeling of the Ind field itself.
            T.comps.__dict__[l] = SingletonType(Ind, a)
    return T
\end{lstlisting}

\autoref{lst:combine} defines the function for combining record types in the fashion required: conflict-free merging and subsequent dependum re-referencing.

\begin{lstlisting}[label={lst:combine},caption={combine}]
def combine_beliefs(bel):
    """Combine a list of belief record types into one."""
    return unsingleton(use_ind_field_labels(reduce(merge_unconflict, bel, RecType())))
\end{lstlisting}

\paragraph{Subtype-relabeling}

The condition described in \autoref{ssec:languagevqa} necessitated the ability to find a relabeling $\eta$ that would fulfill a subtype relation $S \sqsubseteq Q_\eta$.
A simple approach to finding such a relabeling is to list every possible combination and one-by-one perform relabeling and check subtypeness.
However, that is computationally very expensive, and not very interesting even if speed is not a priority.

Another quicker approach is to not perform the relabeling in all steps.
Subtypeness can instead be checked field-wise:
For every field $\langle\ell_Q, T_Q\rangle$ in $Q$, if there is a field $\langle\ell_S, T_S\rangle$ in $S$ such that $T_S \sqsubseteq T_Q$, then let $\eta(\ell_Q) = \ell_S$.
If $\eta$ covers all fields in $Q$, then it follows that $S \sqsubseteq Q_\eta$.

However, we need to mind the types that are dependent on other fields.
For example, $\text{dog}(\text{x}) \cancel{\sqsubseteq} \text{dog}(\text{y})$ even if both $\text{x}:Ind$ and $\text{y}:Ind$ are fields in the respective record types.
To remedy this, the simple approach mentioned above is carried out as a first step, but for basic-type fields only.
Then, for every basic-field relabeling, the second approach is carried out.
In the example case, some of the basic-type relabelings will then have relabeled $\text{y}$ to $\text{x}$ so that the field-wise check becomes $\text{dog}(\text{x}) \sqsubseteq \text{dog}(\text{x})$.

\begin{lstlisting}[label=lst:subtyperlb, caption=Subtype-relabeling.]
from itertools import permutations, combinations

def find_subtype_relabeling(T, U):
    '''Could record type T be a sub type of record type U if relabeling in T is allowed?'''
    # Find possible relabelings for basic-type fields
    basic_label_permutations = set(ps[:len(basic_fields(U))] for ps in permutations(basic_fields(T)))
    
    for tks in basic_label_permutations:
        # Copy U and try a basic-fields relabeling
        U2 = copy_rectype(U)
        rlb = dict(zip(basic_fields(U), tks))
        rectype_relabels(U2, rlb)
        
        # For each U field, find a T field that is a subtype
        match = dict()
        for uk in nonbasic_fields(U2):
            for tk in nonbasic_fields(T):
                if T.comps.__dict__[tk].subtype_of(U2.comps.__dict__[uk]):
                    match[uk] = tk
                    break
            if uk not in match:
                break

        # Successful if all non-basic fields match.
        if len(match) == len(nonbasic_fields(U2)):
            return dict(**rlb, **match)
    return None
\end{lstlisting}



\subsubsection{Language parsing}

Parsing to PyTTR cannot be done directly.
The output of NLTK feature grammars is either strings or \gls{fol} expressions.
For the need of variable substitution, we choose the latter.
The resulting \gls{fol} conjunction expression is then translated to PyTTR ptypes, and $Ind$ fields are created from the arguments of the predicates.

\begin{lstlisting}[label=lst:grammar, caption=Basic parsing of natural language into PyTTR object]
import nltk

grammar = nltk.grammar.FeatureGrammar.fromstring(r'''
%start S
S[SEM=<?s(x) & ?vp(x, y)>] -> NP[SEM=?s] VP[SEM=?vp]
S[SEM=?q] -> QS[SEM=?q]
QS[SEM=<?s(x) & ?vp(x, y)>] -> 'is' 'there' NP[SEM=?s] PP[SEM=?vp]
NP[SEM=<?det(?n)>] -> Det[SEM=?det] N[SEM=?n]
Det[SEM=<\P a.P(a)>] -> 'a' | 'an'
N[SEM=<dog>] -> 'dog'
N[SEM=<car>] -> 'car'
N[SEM=<person>] -> 'person'
N[SEM=<bicycle>] -> 'bicycle'
N[SEM=<backpack>] -> 'backpack'
VP[SEM=?pp] -> 'is' PP[SEM=?pp]
PP[SEM=<\a b.(?prep(a, b) & ?o(b))>] -> Prep[SEM=?prep] NP[SEM=?o]
Prep[SEM=<left>] -> 'to' 'the' 'left' 'of'
Prep[SEM=<right>] -> 'to' 'the' 'right' 'of'
Prep[SEM=<above>] -> 'above'
Prep[SEM=<under>] -> 'under'
''')
parser = nltk.FeatureChartParser(grammar)

def fopc_to_pyttr(expr, T=RecType()):
    """Turns a FOPC object into a RecType."""
    from nltk.sem.logic import ApplicationExpression, AndExpression
    if isinstance(expr, ApplicationExpression):
        pred, args = expr.uncurry()
        T.addfield(gensym('c'), mkptype(str(pred), vars=[str(a) for a in args]))
        for x in args:
            if str(x) not in T.comps.__dict__:
                T.addfield(str(x), Ind)
    if isinstance(expr, AndExpression):
        fopc_to_pyttr(expr.first, T)
        fopc_to_pyttr(expr.second, T)
    return T

def eng_to_pyttr(text):
    trees = parser.parse(text.lower().strip('.?!').split())
    sem = nltk.sem.root_semrep(list(trees)[0])
    T = fopc_to_pyttr(sem)
    return T
\end{lstlisting}



\subsection{Discussion}
\label{sec:discussion}

A \gls{ttr} model was developed, combining perception and language in a \gls{vqa} setting.
The model was then implemented in Python using PyTTR.
External systems and basic implementations were used for object detection, spatial classification and natural-language parsing.
The semantic representation framework, constituted by the \gls{ttr} model, then functions as a bridge between the perceptual systems.
With this, the aims formulated in the introduction were reached.

% Model

As discussed in \autoref{sec:method-spatrel}, our treatment of spatial relations excludes the intrinsical meaning and functional aspects, in favor of model simplicity and easy implementation.
...

The model is restricted to a limited type of polar questions.
Questions such as ``Is the dog brown?'' or ``What is to the left of the car?''
Question types.
Not a full VQA solution.
It can only answer one question type.
With only the extension of parsing, it could understand (= educe situation record types) more complex forms like "A R1 B1 and R2 B2", "A1 R1 B1 and A2 R2 B2". "A is between B1 and B2"
"What is R B?"
"What color is the A?"
"How many A are there?" etc.

%The agent is vaguely formulated?

% Implementation


The implementation lacks coverage of the enclosing agent structure.
...



%Contradiction of Logan \& Sadler's "evidence" for their "theory of apprehension" (which is different from mine)? (Already Regier \& Carlson did.)
%[no difference -sd]
