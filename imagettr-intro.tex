\glsresetall
\section{Introduction}
\label{sec:intro}

Having computers understand visual input is desirable in several areas.
A domestic assistant robot may use a camera to navigate and identify useful objects in a home.
Driver-less cars need to be able to read road signs and track other moving vehicles.
Web crawlers may extract information from images alongside text on the web.

This kind of understanding involves processing sensory (such as visual) input on a cognitive level.
Low-level image processing may include tasks such as prominent color extraction, edge detection and visual pattern recognition.
Higher-level processing, however, includes identifying objects, their properties and their relations to each other.
This information can then be used for language understanding, reasoning, prediction and other cognitive processes.
Making the connection between sensory input and cognitive categories is what concerns the field of \textit{perceptual semantics} \citep{PustejovskyPerceptualsemanticsconstruction1990}.

Humans use language to communicate information.
Thus it is useful to add linguistic capacities to a perceptual system.
With vision and language connected, a robot can talk about what it sees, and descriptions can be automatically generated for images found on the web.
Image caption generation is indeed a popular task for evaluating computer vision systems.
Another one is \textit{\gls{vqa}} \citep{AgrawalVQAVisualQuestion2015}, where the system is expected to generate answers to natural-language questions in the context of a given image.

The connection between different modes of information, such as vision and language, requires a model of semantic representation.
Formal models such as \gls{fol} have long been of choice, but recent developments have seen data-driven approaches excel in some cases.
Briefly put, the former kind tends to deliver deep structures of information in narrow domains, while the latter more easily covers wide domains, but with shallow information content \citep{Dobnik:2017ag}.
A recent contribution that combines several branches in formal systems is \textit{\gls{ttr}} \citep{CooperAustiniantruthattitudes2005,CooperTypetheorylanguage2016}.
\Gls{ttr} is implemented in Python as \textit{PyTTR} \citep{pyttr}.

\subsection{Contribution of this thesis}
\label{sec:contribution}

The main purpose of this thesis is to extend the basic implementation of \gls{ttr} (PyTTR, \cite{pyttr}) to apply it for the first time in a practical task relating vision and language, in particular \gls{vqa}.

The questions that this research raises are:

\begin{enumerate}
\item To what degree is \textit{(a)} \gls{ttr} as a theoretical framework and \textit{(b)} its existing practical implementation suited to connect existing vision and language systems?
\item What are the benefits of using \gls{ttr} this way for \textit{(a)} vision and language systems and \textit{(b)} visual question answering?
\item What can connecting vision and language systems tell us about semantics (and \gls{ttr})?
\end{enumerate}

To explore these questions, a model will be formulated in \gls{ttr} and implemented using PyTTR.
The model will benefit from builing on past proposals; especially relevant are \cite{ttrspat} and \cite{lspc}.
As a limitation for the \gls{vqa} task, the language domain is restricted to polar (yes/no) questions.

%TODO Should I add a discussion on these requirements and limitations? Where?

The theoretical background is summarized in \autoref{sec:background}.
In \autoref{sec:method}, the strategies and techniques used for the implementation are described.
The implementation is then presented in \autoref{sec:results}.
In \autoref{sec:discussion}, the results are discussed in relation to the questions above.
Finally, some conclusions are made in \autoref{sec:conclusions}.
