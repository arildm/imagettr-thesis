\glsresetall
\section{Introduction}
\label{sec:intro}

Having computers ``understand'' images is desirable in several areas.
In this context, understanding means to process visual input on a cognitive level.
Sub-cognitive image processing may include tasks such as prominent color extraction, edge detection and visual pattern recognition.
Cognitive processing, however, includes identifying objects, their properties and their relations to each other, as well as including this information in further cognitive processes, such as reasoning and prediction [sources?].
Making the connection between sensory (such as visual) input and cognitive categories is what concerns the field of \textit{perceptual semantics}.

[``Image understanding''] is interesting in various applications.
It is required in domestic assistant, driver-less cars and other camera-equipped (often mobile) robots, in order to obtain knowledge (or beliefs) and select appropriate actions.
The same kind of processing can also be used outside situated agents, for automatic processing of large sets of visual data [?].

[language, image captions, vqa]

[the creation of ttr]

[research question]
The goal of this thesis is to implement visual perception, cognition of spatial relations and basic natural language understanding in \gls{ttr}.
The implementation shall build on past relevant \gls{ttr} model proposals and focus on high-level completeness, meaning connecting vision and language through a cognitive model using \gls{ttr} as far as possible.
These are the points on which the result is finally qualitatively evaluated.
Lower level parts shall use existing solutions or basic placeholder implementations.

[the purpose of this model: connect perception to formal semantics]

The theoretical background for this thesis is summarized in \textit{Background}.
In the \textit{Method} section, a model is defined and its implementation in Python and PyTTR described.
Then follows an account of the \textit{Results} with discussion, and finally some \textit{Conclusions}.

"[vqa] is our use-case scenario"